# matsushibadenki/snn/app/langchain_main.py
# LangChainã¨é€£æºã—ãŸSNNãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
#
# æ©Ÿèƒ½:
# - ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— ãƒ•ã‚§ãƒ¼ã‚º2ã€Œ2.4. ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—é–‹ç™ºã€ã«å¯¾å¿œã€‚
# - DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰SNNLangChainAdapterã‚’å–å¾—ã€‚
# - LangChainã®PromptTemplateã¨LLMChainã‚’åˆ©ç”¨ã—ã¦ã€ã‚ˆã‚Šæ§‹é€ åŒ–ã•ã‚ŒãŸå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¢ã€‚
# - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã«å¯¾å¿œã€‚

import gradio as gr
import argparse
import sys
from pathlib import Path
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from typing import Iterator

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).resolve().parent.parent))

from app.containers import AppContainer

def main():
    parser = argparse.ArgumentParser(description="SNN + LangChain é€£æºAIãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--model_path", type=str, help="ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ (è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã)")
    args = parser.parse_args()

    # DIã‚³ãƒ³ãƒ†ãƒŠã‚’åˆæœŸåŒ–
    container = AppContainer()
    container.config.from_yaml(args.config)
    if args.model_path:
        container.config.model.path.from_value(args.model_path)

    # ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰LangChainã‚¢ãƒ€ãƒ—ã‚¿ã‚’å–å¾—
    snn_llm = container.langchain_adapter()
    print(f"Loading SNN model from: {container.config.model.path()}")
    print("âœ… SNN model loaded and wrapped for LangChain successfully.")

    # LangChainã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®šç¾©
    template = """
    ã‚ãªãŸã¯ã€ç°¡æ½”ã§å½¹ç«‹ã¤ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚

    è³ªå•: {question}
    å›ç­”:
    """
    prompt = PromptTemplate(template=template, input_variables=["question"])

    # LLMChainã‚’ä½œæˆ
    llm_chain = LLMChain(prompt=prompt, llm=snn_llm)

# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    def handle_message(message: str, history: list) -> Iterator[str]:
        """Gradioã‹ã‚‰ã®å…¥åŠ›ã‚’å‡¦ç†ã—ã€LLMChainã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œã—ã¦å¿œç­”ã‚’è¿”ã™"""
        print("-" * 30)
        print(f"Input question to LLMChain: {message}")
        
        full_response = ""
        # LangChainã®streamãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        for chunk in llm_chain.stream({"question": message}):
            # streamã¯è¾æ›¸ã‚’è¿”ã™ã®ã§ã€'text'ã‚­ãƒ¼ã®å€¤ã‚’å–å¾—
            text_chunk = chunk.get('text', '')
            full_response += text_chunk
            yield full_response

        print(f"Generated answer: {full_response.strip()}")
        print("-" * 30)
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

    # Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®æ§‹ç¯‰
    chatbot_interface = gr.ChatInterface(
        fn=handle_message,
        title="ğŸ¤– SNN + LangChain Prototype (Streaming)",
        description="SNNãƒ¢ãƒ‡ãƒ«ã‚’LangChainçµŒç”±ã§åˆ©ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã€‚è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        chatbot=gr.Chatbot(height=500),
        textbox=gr.Textbox(placeholder="è³ªå•ã‚’å…¥åŠ›...", container=False, scale=7),
        retry_btn=None,
        undo_btn="å‰Šé™¤",
        clear_btn="ã‚¯ãƒªã‚¢",
    )
    
    # Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•
    server_port = container.config.app.server_port() + 1 # ãƒãƒ¼ãƒˆãŒè¡çªã—ãªã„ã‚ˆã†ã«+1ã™ã‚‹
    print("\nStarting Gradio web server for LangChain app...")
    print(f"Please open http://{container.config.app.server_name()}:{server_port} in your browser.")
    chatbot_interface.launch(
        server_name=container.config.app.server_name(),
        server_port=server_port,
    )

if __name__ == "__main__":
    main()
