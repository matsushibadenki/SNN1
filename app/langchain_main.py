# matsushibadenki/snn/app/langchain_main.py
# LangChainã¨é€£æºã—ãŸSNNãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
#
# æ©Ÿèƒ½:
# - ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— ãƒ•ã‚§ãƒ¼ã‚º2ã€Œ2.4. ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—é–‹ç™ºã€ã«å¯¾å¿œã€‚
# - DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰SNNLangChainAdapterã‚’å–å¾—ã€‚
# - LangChainã®PromptTemplateã¨LLMChainã‚’åˆ©ç”¨ã—ã¦ã€ã‚ˆã‚Šæ§‹é€ åŒ–ã•ã‚ŒãŸå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¢ã€‚
# - Gradio Blocksã‚’ä½¿ç”¨ã—ã¦ã€ãƒãƒ£ãƒƒãƒˆç”»é¢ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆæƒ…å ±ãƒ‘ãƒãƒ«ã‚’æŒã¤UIã‚’æ§‹ç¯‰ã€‚
# - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã«å¯¾å¿œã—ã€SNNã®è¨ˆç®—çµ±è¨ˆã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤ºã€‚

import gradio as gr
import argparse
import sys
import time
from pathlib import Path
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from typing import Iterator, Tuple, List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).resolve().parent.parent))

from app.containers import AppContainer

def main():
    parser = argparse.ArgumentParser(description="SNN + LangChain é€£æºAIãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--model_path", type=str, help="ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ (è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã)")
    args = parser.parse_args()

    # DIã‚³ãƒ³ãƒ†ãƒŠã‚’åˆæœŸåŒ–
    container = AppContainer()
    container.config.from_yaml(args.config)
    if args.model_path:
        container.config.model.path.from_value(args.model_path)

    # ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰LangChainã‚¢ãƒ€ãƒ—ã‚¿ã‚’å–å¾—
    snn_llm = container.langchain_adapter()
    print(f"Loading SNN model from: {container.config.model.path()}")
    print("âœ… SNN model loaded and wrapped for LangChain successfully.")

    # LangChainã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®šç¾©
    template = """
    ã‚ãªãŸã¯ã€ç°¡æ½”ã§å½¹ç«‹ã¤ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚

    è³ªå•: {question}
    å›ç­”:
    """
    prompt = PromptTemplate(template=template, input_variables=["question"])

    # LLMChainã‚’ä½œæˆ
    llm_chain = LLMChain(prompt=prompt, llm=snn_llm)

    # ã‚¢ãƒã‚¿ãƒ¼ç”¨ã®SVGã‚¢ã‚¤ã‚³ãƒ³ã‚’å®šç¾©
    user_avatar_svg = r"""
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-user"><path d="M20 21v-2a4 4 0 0 0-4-4H8a4 4 0 0 0-4 4v2"></path><circle cx="12" cy="7" r="4"></circle></svg>
    """
    assistant_avatar_svg = r"""
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-zap"><polygon points="13 2 3 14 12 14 11 22 21 10 12 10 13 2"></polygon></svg>
    """

    def stream_response(message: str, history: List[List[str]]) -> Iterator[Tuple[List[List[str]], str]]:
        """Gradioã®Blocks UIã®ãŸã‚ã«ã€ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã¨çµ±è¨ˆæƒ…å ±ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã™ã‚‹ã€‚"""
        history.append([message, ""])
        
        print("-" * 30)
        print(f"Input question to LLMChain: {message}")
        
        start_time = time.time()
        
        full_response = ""
        token_count = 0
        
        # LangChainã®streamãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        for chunk in llm_chain.stream({"question": message}):
            # chunkã¯è¾æ›¸å½¢å¼ãªã®ã§ã€ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’å–ã‚Šå‡ºã™
            response_piece = chunk.get('text', '')
            full_response += response_piece
            token_count += 1
            history[-1][1] = full_response
            
            duration = time.time() - start_time
            # LangChainã‚¢ãƒ€ãƒ—ã‚¿çµŒç”±ã§SNNã‚¨ãƒ³ã‚¸ãƒ³ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
            stats = snn_llm.snn_engine.last_inference_stats
            total_spikes = stats.get("total_spikes", 0)
            spikes_per_second = total_spikes / duration if duration > 0 else 0
            tokens_per_second = token_count / duration if duration > 0 else 0

            stats_md = f"""
            **Inference Time:** `{duration:.2f} s`
            **Tokens/Second:** `{tokens_per_second:.2f}`
            ---
            **Total Spikes:** `{total_spikes:,.0f}`
            **Spikes/Second:** `{spikes_per_second:,.0f}`
            """
            
            yield history, stats_md

        # Final log to console
        duration = time.time() - start_time
        stats = snn_llm.snn_engine.last_inference_stats
        total_spikes = stats.get("total_spikes", 0)
        print(f"\nGenerated response: {full_response.strip()}")
        print(f"Inference time: {duration:.4f} seconds")
        print(f"Total spikes: {total_spikes:,.0f}")
        print("-" * 30)

    # Gradio Blocks ã‚’ä½¿ç”¨ã—ã¦UIã‚’æ§‹ç¯‰
    with gr.Blocks(theme=gr.themes.Soft(primary_hue="green", secondary_hue="lime")) as demo:
        gr.Markdown(
            """
            # ğŸ¤– SNN + LangChain Prototype
            SNNãƒ¢ãƒ‡ãƒ«ã‚’LangChainçµŒç”±ã§åˆ©ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã€‚
            å³å´ã®ãƒ‘ãƒãƒ«ã«ã¯ã€æ¨è«–æ™‚é–“ã‚„ç·ã‚¹ãƒ‘ã‚¤ã‚¯æ•°ãªã©ã®çµ±è¨ˆæƒ…å ±ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
            """
        )
        
        initial_stats_md = """
        **Inference Time:** `N/A`
        **Tokens/Second:** `N/A`
        ---
        **Total Spikes:** `N/A`
        **Spikes/Second:** `N/A`
        """

        with gr.Row():
            with gr.Column(scale=2):
                chatbot = gr.Chatbot(label="SNN+LangChain Chat", height=500, avatar_images=(user_avatar_svg, assistant_avatar_svg))
            with gr.Column(scale=1):
                stats_display = gr.Markdown(value=initial_stats_md, label="ğŸ“Š Inference Stats")

        with gr.Row():
            msg_textbox = gr.Textbox(
                show_label=False,
                placeholder="è³ªå•ã‚’å…¥åŠ›...",
                container=False,
                scale=6,
            )
            submit_btn = gr.Button("Send", variant="primary", scale=1)
            clear_btn = gr.Button("Clear", scale=1)

        gr.Markdown("<footer><p>Â© 2025 SNN System Design Project. All rights reserved.</p></footer>")

        def clear_all():
            """ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã€ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã€çµ±è¨ˆè¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢ã™ã‚‹"""
            return [], "", initial_stats_md

        # `submit` ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å®šç¾©
        submit_event = msg_textbox.submit(
            fn=stream_response,
            inputs=[msg_textbox, chatbot],
            outputs=[chatbot, stats_display]
        )
        submit_event.then(fn=lambda: "", inputs=None, outputs=msg_textbox)
        
        button_submit_event = submit_btn.click(
            fn=stream_response,
            inputs=[msg_textbox, chatbot],
            outputs=[chatbot, stats_display]
        )
        button_submit_event.then(fn=lambda: "", inputs=None, outputs=msg_textbox)

        # `clear` ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å®šç¾©
        clear_btn.click(
            fn=clear_all,
            inputs=None,
            outputs=[chatbot, msg_textbox, stats_display],
            queue=False
        )
    
    # Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•
    server_port = container.config.app.server_port() + 1 # ãƒãƒ¼ãƒˆãŒè¡çªã—ãªã„ã‚ˆã†ã«+1ã™ã‚‹
    print("\nStarting Gradio web server for LangChain app...")
    print(f"Please open http://{container.config.app.server_name()}:{server_port} in your browser.")
    demo.launch(
        server_name=container.config.app.server_name(),
        server_port=server_port,
    )

if __name__ == "__main__":
    main()

