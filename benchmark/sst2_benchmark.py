# matsushibadenki/snn/benchmark/sst2_benchmark.py
#
# GLUEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (SST-2ã‚¿ã‚¹ã‚¯) ã‚’ç”¨ã„ãŸSNNãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
#
# ç›®çš„:
# - ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— ãƒ•ã‚§ãƒ¼ã‚º1ã€Œ1.1. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç’°å¢ƒã®æ§‹ç¯‰ã€ã«å¯¾å¿œã€‚
# - æ¨™æº–çš„ãªNLPã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å®¢è¦³çš„ã‹ã¤å®šé‡çš„ã«è©•ä¾¡ã™ã‚‹åŸºç›¤ã‚’ç¢ºç«‹ã™ã‚‹ã€‚
#
# æ©Ÿèƒ½:
# 1. Hugging Face `datasets`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰SST-2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‚
# 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’SNNãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’å¯èƒ½ãª.jsonlå½¢å¼ã«å‰å‡¦ç†ãƒ»å¤‰æ›ã€‚
# 3. main.pyã®å­¦ç¿’ãƒ»æ¨è«–ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‘¼ã³å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡ã‚’å®Ÿè¡Œã€‚
# 4. scikit-learnã‚’ä½¿ç”¨ã—ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ­£è§£ç‡(Accuracy)ã‚’è¨ˆç®—ãƒ»è¡¨ç¤ºã€‚

import os
import json
from datasets import load_dataset
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import sys
import torch

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’sys.pathã«è¿½åŠ ã—ã¦ã€mainã‚„snn_coreã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ã«ã™ã‚‹
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from main import run_training, SNNInferenceEngine
from snn_core import Vocabulary

def prepare_sst2_data(output_dir: str = "data"):
    """
    SST-2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€.jsonlå½¢å¼ã«å¤‰æ›ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    print("Downloading SST-2 dataset...")
    dataset = load_dataset("glue", "sst2")

    data_paths = {}
    for split in ["train", "validation", "test"]:
        jsonl_path = os.path.join(output_dir, f"sst2_{split}.jsonl")
        data_paths[split] = jsonl_path
        
        print(f"Processing '{split}' split -> {jsonl_path}")
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for example in tqdm(dataset[split]):
                # ãƒ©ãƒ™ãƒ«ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ï¼ˆå­¦ç¿’æ™‚ã«ã¯ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã§èªå½™ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ï¼‰
                # ã“ã“ã§ã¯ç°¡å˜åŒ–ã®ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ©ãƒ™ãƒ«ã‚’çµåˆã—ãŸå½¢å¼ã«ã¯ã›ãšã€
                # instructionå½¢å¼ã¨ã—ã¦æ‰±ã†
                instruction = "Classify the sentiment of the following sentence."
                output = "positive" if example['label'] == 1 else "negative"
                
                record = {
                    "instruction": instruction,
                    "input": example['sentence'],
                    "output": output
                }
                f.write(json.dumps(record) + "\n")
    
    print("âœ… SST-2 data preparation complete.")
    return data_paths

def run_benchmark(data_paths: dict, model_path: str):
    """
    æº–å‚™ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡ã‚’è¡Œã†ã€‚
    """
    print("\nğŸš€ Starting SST-2 Benchmark...")

    # --- 1. Training ---
    print("\nğŸ”¥ Step 1: Training the model on SST-2 train set...")
    # main.run_trainingã‚’å‘¼ã³å‡ºã™ãŸã‚ã®æ“¬ä¼¼çš„ãªå¼•æ•°ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    train_args = type('Args', (), {
        'data_path': data_paths['train'],
        'data_format': 'instruction',
        'epochs': 5, # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ãŸã‚ã‚¨ãƒãƒƒã‚¯æ•°ã¯å°‘ãªãè¨­å®š
        'batch_size': 16,
        'learning_rate': 1e-4,
        'log_interval': 1,
        'model_path': model_path,
        'd_model': 64,
        'd_state': 32,
        'num_layers': 2,
        'time_steps': 16
    })()
    
    vocab = run_training(train_args)
    print("âœ… Model training complete.")

    # --- 2. Evaluation ---
    print("\nğŸ“Š Step 2: Evaluating the model on SST-2 validation set...")
    engine = SNNInferenceEngine(model_path=model_path)
    
    true_labels = []
    pred_labels = []

    validation_data = []
    with open(data_paths['validation'], 'r', encoding='utf-8') as f:
        for line in f:
            validation_data.append(json.loads(line))

    for item in tqdm(validation_data, desc="Evaluating"):
        prompt = f"{item['instruction']}\n{item['input']}"
        generated_text = engine.generate(prompt, max_len=3) # "positive" or "negative"
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã‚’æ±ºå®š
        if "positive" in generated_text:
            predicted_label = "positive"
        elif "negative" in generated_text:
            predicted_label = "negative"
        else:
            predicted_label = "unknown" # ç”ŸæˆãŒã†ã¾ãã„ã‹ãªã‹ã£ãŸå ´åˆ

        true_labels.append(item['output'])
        pred_labels.append(predicted_label)
        
    # --- 3. Calculate Metrics ---
    accuracy = accuracy_score(true_labels, pred_labels)
    
    print("\nğŸ‰ Benchmark Results:")
    print("=" * 30)
    print(f"  Validation Accuracy: {accuracy:.4f}")
    print("=" * 30)

if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    prepared_data_paths = prepare_sst2_data()
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    output_model_path = "breakthrough_snn_sst2.pth"
    run_benchmark(prepared_data_paths, output_model_path)