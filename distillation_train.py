# /path/to/your/project/distillation_train.py
# çŸ¥è­˜è’¸ç•™ã‚’ç”¨ã„ãŸSNNãƒ¢ãƒ‡ãƒ«ã®åˆ†æ•£å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
#
# ç›®çš„:
# - ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— ãƒ•ã‚§ãƒ¼ã‚º2ã€Œ2.2. çŸ¥è­˜è’¸ç•™ã®æœ¬æ ¼å°å…¥ã€ã«å¯¾å¿œã€‚
# - å¤§è¦æ¨¡ANNï¼ˆæ•™å¸«ï¼‰ã®çŸ¥è­˜ã‚’SNNï¼ˆç”Ÿå¾’ï¼‰ã«è»¢ç§»ã•ã›ã€å­¦ç¿’ã‚’åŠ¹ç‡åŒ–ãƒ»é«˜æ€§èƒ½åŒ–ã™ã‚‹ã€‚
#
# å®Ÿè¡Œæ–¹æ³•:
# torchrun --nproc_per_node=<NUM_GPUS> distillation_train.py <DATA_PATH> [OPTIONS]
# ä¾‹: torchrun --nproc_per_node=2 distillation_train.py data/wikitext-103_train.jsonl --epochs 5

import os
import argparse
import json
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoTokenizer, AutoModelForCausalLM

from main import Vocabulary, set_seed
from snn_core import BreakthroughSNN
from knowledge_distillation import DistillationLoss, DistillationTrainer

def setup_distributed(rank: int, world_size: int):
    """åˆ†æ•£å­¦ç¿’ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã€‚"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup():
    dist.destroy_process_group()

# --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨Collate Function ---
class DistillationDataset(Dataset):
    def __init__(self, file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            self.data = [json.loads(line)['text'] for line in f if line.strip()]
    def __len__(self): return len(self.data)
    def __getitem__(self, idx): return self.data[idx]

def distillation_collate_fn(batch, student_vocab, teacher_tokenizer, device):
    """æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã§ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’è¡Œã†Collate Functionã€‚"""
    raw_texts = batch
    
    # 1. ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ‡ãƒ¼ã‚¿
    student_inputs, student_targets = [], []
    for text in raw_texts:
        encoded = student_vocab.encode(text)
        student_inputs.append(torch.tensor(encoded[:-1]))
        student_targets.append(torch.tensor(encoded[1:], dtype=torch.long))
    
    student_padded_inputs = pad_sequence(student_inputs, batch_first=True, padding_value=student_vocab.pad_id)
    student_padded_targets = pad_sequence(student_targets, batch_first=True, padding_value=student_vocab.pad_id)

    # 2. æ•™å¸«ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ‡ãƒ¼ã‚¿
    teacher_tokenized = teacher_tokenizer(raw_texts, padding=True, truncation=True, max_length=128, return_tensors="pt")
    
    return (
        student_padded_inputs,
        student_padded_targets,
        teacher_tokenized.input_ids,
        teacher_tokenized.attention_mask
    )

def main_worker(rank: int, world_size: int, args: argparse.Namespace):
    """å„GPUãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¡ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ã€‚"""
    print(f"Running Distillation Training on rank {rank}.")
    setup_distributed(rank, world_size)
    set_seed(args.seed)

    # --- æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨Tokenizerã®æº–å‚™ ---
    if rank == 0:
        print(f"Loading teacher model: {args.teacher_model}...")
    tokenizer = AutoTokenizer.from_pretrained(args.teacher_model)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    teacher_model = AutoModelForCausalLM.from_pretrained(args.teacher_model)
    print(f"âœ… [Rank {rank}] Teacher model loaded.")

    # --- ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ç”¨ã®èªå½™æº–å‚™ ---
    vocab_path = "vocab_distill.pth"
    if rank == 0:
        vocab = Vocabulary()
        print("ğŸ“– [Rank 0] Building student vocabulary...")
        with open(args.data_path, 'r', encoding='utf-8') as f:
            texts = (json.loads(line)['text'] for line in f)
            vocab.build_vocab(texts)
        torch.save(vocab, vocab_path)
        print(f"âœ… [Rank 0] Student vocabulary built. Size: {vocab.vocab_size}")
    
    dist.barrier()
    vocab = torch.load(vocab_path, map_location='cpu')
    
    # --- ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™ ---
    dataset = DistillationDataset(args.data_path)
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)
    custom_collate = lambda batch: distillation_collate_fn(batch, vocab, tokenizer, f"cuda:{rank}")
    dataloader = DataLoader(dataset, batch_size=args.batch_size, sampler=sampler, collate_fn=custom_collate, num_workers=2)

    # --- ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ ---
    student_config = {'d_model': args.d_model, 'd_state': args.d_state, 'num_layers': args.num_layers, 'time_steps': args.time_steps}
    student_model = BreakthroughSNN(vocab_size=vocab.vocab_size, **student_config).to(rank)
    ddp_student_model = DDP(student_model, device_ids=[rank])

    optimizer = torch.optim.AdamW(ddp_student_model.parameters(), lr=args.learning_rate)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs) if args.use_scheduler else None
    criterion = DistillationLoss(student_pad_id=vocab.pad_id)
    
    trainer = DistillationTrainer(
        teacher_model=teacher_model,
        model=ddp_student_model,
        optimizer=optimizer,
        criterion=criterion,
        scheduler=scheduler,
        device=f"cuda:{rank}",
        rank=rank
    )

    # --- å­¦ç¿’ãƒ«ãƒ¼ãƒ— ---
    if rank == 0:
        print("\nğŸ”¥ Knowledge Distillation Training Started...")
    for epoch in range(args.epochs):
        sampler.set_epoch(epoch)
        metrics = trainer.train_epoch(dataloader)
        if rank == 0:
            lr = scheduler.get_last_lr()[0] if scheduler else args.learning_rate
            metrics_str = ", ".join([f"{k}: {v:.4f}" for k, v in metrics.items()])
            print(f"Epoch {epoch+1: >3}/{args.epochs}: {metrics_str}, lr: {lr:.6f}")
            if (epoch + 1) % args.log_interval == 0:
                trainer.save_checkpoint(args.model_path, epoch, vocab=vocab, config=student_config)

    if rank == 0 and os.path.exists(vocab_path):
        os.remove(vocab_path)
    cleanup()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SNNãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜è’¸ç•™")
    parser.add_argument("data_path", type=str, help="å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ (.jsonl, simple_textå½¢å¼)")
    parser.add_argument("--teacher_model", type=str, default="gpt2", help="Hugging Faceã®æ•™å¸«ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--batch_size", type=int, default=8, help="å„GPUã‚ãŸã‚Šã®ãƒãƒƒãƒã‚µã‚¤ã‚º")
    parser.add_argument("--learning_rate", type=float, default=3e-4)
    parser.add_argument("--log_interval", type=int, default=1)
    parser.add_argument("--model_path", type=str, default="snn_distilled_model.pth")
    parser.add_argument("--d_model", type=int, default=256)
    parser.add_argument("--d_state", type=int, default=128)
    parser.add_argument("--num_layers", type=int, default=8)
    parser.add_argument("--time_steps", type=int, default=20)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--use_scheduler", action='store_true')
    args = parser.parse_args()

    world_size = int(os.environ.get("WORLD_SIZE", 1))
    rank = int(os.environ.get("RANK", 0))
    main_worker(rank, world_size, args)
