# /path/to/your/project/main.py
# SNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
#
# å…ƒãƒ•ã‚¡ã‚¤ãƒ«:
# - train_text_snn.py (å­¦ç¿’éƒ¨åˆ†)
# - inference.py (æ¨è«–éƒ¨åˆ†)
# - snn_breakthrough.py (å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯)
# ã‚’çµ±åˆã—ã€snn_core.pyã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã€‚
# å­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’ã€Œæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã€ã«ä¿®æ­£ã—ã€ã‚ˆã‚Šé«˜åº¦ãªãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã€‚
#
# æ”¹å–„ç‚¹:
# - argparseã‚’å°å…¥ã—ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«(JSON/TXT)ã‚’èª­ã¿è¾¼ã‚ã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚
# - æ±ç”¨åŒ–ã•ã‚ŒãŸBreakthroughTrainerã«å¯¾å¿œã€‚

import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import itertools
from typing import List, Tuple
import os
import random
import argparse
import json

# snn_coreã‹ã‚‰ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from snn_core import BreakthroughSNN, BreakthroughTrainer, CombinedLoss

# ----------------------------------------
# 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨èªå½™ã®æ§‹ç¯‰
# ----------------------------------------

def load_data_from_file(file_path: str, json_key: str = None) -> List[str]:
    """
    å¤–éƒ¨ã®JSONã¾ãŸã¯TXTãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

    Args:
        file_path (str): ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
        json_key (str, optional): JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ã‚­ãƒ¼ã€‚

    Returns:
        List[str]: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã€‚
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")

    _, ext = os.path.splitext(file_path)
    
    if ext == '.json':
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if json_key:
            if json_key not in data:
                raise KeyError(f"æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ '{json_key}' ãŒJSONãƒ•ã‚¡ã‚¤ãƒ«å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            texts = data[json_key]
        else:
            # ã‚­ãƒ¼ãŒæŒ‡å®šã•ã‚Œãªã„å ´åˆã€JSONãƒ‡ãƒ¼ã‚¿è‡ªä½“ãŒãƒªã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’æœŸå¾…
            texts = data
        
        if not isinstance(texts, list) or not all(isinstance(t, str) for t in texts):
            raise TypeError("JSONã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ã€æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
        return texts

    elif ext == '.txt':
        with open(file_path, 'r', encoding='utf-8') as f:
            # ç©ºè¡Œã‚’é™¤å¤–ã—ã¦èª­ã¿è¾¼ã‚€
            return [line.strip() for line in f if line.strip()]
    else:
        raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™: {ext} (.json ã¾ãŸã¯ .txt ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„)")

class Vocabulary:
    """ãƒ†ã‚­ã‚¹ãƒˆã¨IDã‚’ç›¸äº’å¤‰æ›ã™ã‚‹ãŸã‚ã®èªå½™ã‚¯ãƒ©ã‚¹"""
    def __init__(self, all_texts: List[str]):
        # äºˆç´„ãƒˆãƒ¼ã‚¯ãƒ³
        self.special_tokens = {"<PAD>": 0, "<UNK>": 1, "<START>": 2, "<END>": 3}
        self.word2idx = self.special_tokens.copy()
        self.idx2word = {v: k for k, v in self.word2idx.items()}
        if all_texts:
            self._build_vocab(all_texts)

    def _build_vocab(self, all_texts: List[str]):
        all_words = list(itertools.chain.from_iterable(txt.lower().split() for txt in all_texts))
        word_counts = Counter(all_words)
        for word, _ in word_counts.items():
            if word not in self.word2idx:
                idx = len(self.word2idx)
                self.word2idx[word] = idx
                self.idx2word[idx] = word
    
    def encode(self, text: str, add_start_end: bool = True) -> List[int]:
        tokens = [self.word2idx.get(word.lower(), self.special_tokens["<UNK>"]) for word in text.split()]
        if add_start_end:
            return [self.special_tokens["<START>"]] + tokens + [self.special_tokens["<END>"]]
        return tokens

    def decode(self, token_ids: List[int]) -> str:
        return " ".join([self.idx2word.get(idx, "<UNK>") for idx in token_ids])

    @property
    def vocab_size(self) -> int:
        return len(self.word2idx)
    
    @property
    def pad_id(self) -> int:
        return self.special_tokens["<PAD>"]

class NextTokenPredictionDataset(Dataset):
    """
    æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
    æ–‡ç« ã‚’å—ã‘å–ã‚Šã€(å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹, ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹)ã®ãƒšã‚¢ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    ä¾‹: "i love this movie" -> input="<START> i love this", target="i love this <END>"
    """
    def __init__(self, data: List[str], vocab: Vocabulary, max_len: int = 32):
        self.vocab = vocab
        self.max_len = max_len
        self.encoded_data = [self.vocab.encode(text) for text in data]
    
    def __len__(self):
        return len(self.encoded_data)
    
    def __getitem__(self, idx):
        encoded = self.encoded_data[idx]
        
        # å…¥åŠ›ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä½œæˆ
        input_seq = encoded[:-1]
        target_seq = encoded[1:]
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        input_len = len(input_seq)
        pad_len = self.max_len - input_len
        
        padded_input = input_seq[:self.max_len] + [self.vocab.pad_id] * max(0, pad_len)
        padded_target = target_seq[:self.max_len] + [self.vocab.pad_id] * max(0, pad_len)
        
        return torch.tensor(padded_input), torch.tensor(padded_target, dtype=torch.long)

# ----------------------------------------
# 2. æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
# ----------------------------------------

class SNNInferenceEngine:
    """SNNãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚„åˆ†æã‚’è¡Œã†æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³"""
    def __init__(self, model_path: str, device: str = "cpu"):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        
        self.device = torch.device(device)
        checkpoint = torch.load(model_path, map_location=self.device)
        
        self.vocab = checkpoint['vocab']
        config = checkpoint['config']
        
        self.model = BreakthroughSNN(vocab_size=self.vocab.vocab_size, **config).to(self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
    def generate(self, start_text: str, max_len: int = 20) -> str:
        """ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«ç¶šãæ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹"""
        print(f"\nç”Ÿæˆé–‹å§‹: '{start_text}'")
        
        # åˆæœŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        input_ids = self.vocab.encode(start_text, add_start_end=False)
        input_tensor = torch.tensor([input_ids], device=self.device)
        
        generated_ids = list(input_ids)
        
        with torch.no_grad():
            for _ in range(max_len):
                # ãƒ¢ãƒ‡ãƒ«ã«ç¾åœ¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å…¥åŠ›
                logits = self.model(input_tensor)
                
                # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬ç¢ºç‡ã‹ã‚‰æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                next_token_logits = logits[:, -1, :]
                next_token_id = torch.argmax(next_token_logits, dim=-1).item()
                
                # ç”ŸæˆãŒçµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã«é”ã—ãŸã‚‰çµ‚äº†
                if next_token_id == self.vocab.special_tokens["<END>"]:
                    break
                
                generated_ids.append(next_token_id)
                # æ¬¡ã®å…¥åŠ›ã¨ã—ã¦ã€ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
                input_tensor = torch.tensor([generated_ids], device=self.device)
        
        return self.vocab.decode(generated_ids)

# ----------------------------------------
# 3. å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯
# ----------------------------------------

def train(args):
    """ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®Ÿè¡Œ"""
    print("ğŸš€ é©æ–°çš„SNNã‚·ã‚¹ãƒ†ãƒ ã®è¨“ç·´é–‹å§‹ (æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã‚¿ã‚¹ã‚¯)")
    
    try:
        train_data = load_data_from_file(args.data_path, args.json_key)
        print(f"âœ… {args.data_path} ã‹ã‚‰ {len(train_data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    except (FileNotFoundError, KeyError, TypeError, ValueError) as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\nè©³ç´°: {e}")
        return

    vocab = Vocabulary(train_data)
    print(f"ğŸ“– èªå½™ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚èªå½™æ•°: {vocab.vocab_size}")
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    config = {'d_model': 64, 'd_state': 32, 'num_layers': 2, 'time_steps': 16}
    model = BreakthroughSNN(vocab_size=vocab.vocab_size, **config)
    
    # æ±ç”¨Trainerã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate)
    criterion = CombinedLoss()
    trainer = BreakthroughTrainer(model, optimizer, criterion)
    
    dataset = NextTokenPredictionDataset(train_data, vocab)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    print("\nğŸ”¥ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    for epoch in range(args.epochs):
        metrics = trainer.train_epoch(dataloader)
        if (epoch + 1) % args.log_interval == 0:
            metrics_str = ", ".join([f"{k}: {v:.4f}" for k, v in metrics.items()])
            print(f"Epoch {epoch+1: >3}/{args.epochs}: {metrics_str}")
            
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_path = args.model_path
    torch.save({
        'model_state_dict': model.state_dict(),
        'vocab': vocab,
        'config': config
    }, model_path)
    print(f"\nâœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ '{model_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

def inference(args):
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ï¼ˆæ–‡ç« ç”Ÿæˆï¼‰ã‚’å®Ÿè¡Œ"""
    try:
        engine = SNNInferenceEngine(model_path=args.model_path)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã‚‹ãƒ«ãƒ¼ãƒ—
        print("\nğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¾ãŸã¯ 'quit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        while True:
            start_text = input("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: ")
            if start_text.lower() in ["exit", "quit"]:
                break
            generated_text = engine.generate(start_text, max_len=args.max_len)
            print(f"ç”Ÿæˆçµæœ: {generated_text}")

    except FileNotFoundError as e:
        print(f"âŒ {e}")
        print(f"ã‚¨ãƒ©ãƒ¼: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«({args.model_path})ãŒå¿…è¦ã§ã™ã€‚å…ˆã« 'train' ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        print(f"âŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SNNãƒ™ãƒ¼ã‚¹ AIãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ")
    subparsers = parser.add_subparsers(dest="command", required=True, help="å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒ‰")

    # --- å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰ ---
    parser_train = subparsers.add_parser("train", help="SNNãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã™")
    parser_train.add_argument("data_path", type=str, help="å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.json ã¾ãŸã¯ .txt)")
    parser_train.add_argument("--json_key", type=str, default=None, help="JSONãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ã‚­ãƒ¼")
    parser_train.add_argument("--epochs", type=int, default=100, help="å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°")
    parser_train.add_argument("--batch_size", type=int, default=4, help="ãƒãƒƒãƒã‚µã‚¤ã‚º")
    parser_train.add_argument("--learning_rate", type=float, default=5e-4, help="å­¦ç¿’ç‡")
    parser_train.add_argument("--log_interval", type=int, default=20, help="ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‚¨ãƒãƒƒã‚¯é–“éš”")
    parser_train.add_argument("--model_path", type=str, default="breakthrough_snn_model.pth", help="å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹")
    parser_train.set_defaults(func=train)

    # --- æ¨è«–ã‚³ãƒãƒ³ãƒ‰ ---
    parser_inference = subparsers.add_parser("inference", help="å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ï¼ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼‰ã‚’å®Ÿè¡Œã—ã¾ã™")
    parser_inference.add_argument("--model_path", type=str, default="breakthrough_snn_model.pth", help="å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹")
    parser_inference.add_argument("--max_len", type=int, default=30, help="ç”Ÿæˆã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å¤§é•·")
    parser_inference.set_defaults(func=inference)

    args = parser.parse_args()
    args.func(args)
