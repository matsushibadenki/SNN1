# /path/to/your/project/main.py
# SNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
#
# å…ƒãƒ•ã‚¡ã‚¤ãƒ«:
# - train_text_snn.py (å­¦ç¿’éƒ¨åˆ†)
# - inference.py (æ¨è«–éƒ¨åˆ†)
# - snn_breakthrough.py (å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯)
# ã‚’çµ±åˆã—ã€snn_core.pyã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã€‚
# å­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’ã€Œæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã€ã«ä¿®æ­£ã—ã€ã‚ˆã‚Šé«˜åº¦ãªãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã€‚

import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import itertools
from typing import List, Tuple
import os
import random

# snn_coreã‹ã‚‰ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from snn_core import BreakthroughSNN, BreakthroughTrainer

# ----------------------------------------
# 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨èªå½™ã®æ§‹ç¯‰
# ----------------------------------------

class Vocabulary:
    """ãƒ†ã‚­ã‚¹ãƒˆã¨IDã‚’ç›¸äº’å¤‰æ›ã™ã‚‹ãŸã‚ã®èªå½™ã‚¯ãƒ©ã‚¹"""
    def __init__(self, all_texts: List[str]):
        # äºˆç´„ãƒˆãƒ¼ã‚¯ãƒ³
        self.special_tokens = {"<PAD>": 0, "<UNK>": 1, "<START>": 2, "<END>": 3}
        self.word2idx = self.special_tokens.copy()
        self.idx2word = {v: k for k, v in self.word2idx.items()}
        if all_texts:
            self._build_vocab(all_texts)

    def _build_vocab(self, all_texts: List[str]):
        all_words = list(itertools.chain.from_iterable(txt.lower().split() for txt in all_texts))
        word_counts = Counter(all_words)
        for word, _ in word_counts.items():
            if word not in self.word2idx:
                idx = len(self.word2idx)
                self.word2idx[word] = idx
                self.idx2word[idx] = word
    
    def encode(self, text: str, add_start_end: bool = True) -> List[int]:
        tokens = [self.word2idx.get(word.lower(), self.special_tokens["<UNK>"]) for word in text.split()]
        if add_start_end:
            return [self.special_tokens["<START>"]] + tokens + [self.special_tokens["<END>"]]
        return tokens

    def decode(self, token_ids: List[int]) -> str:
        return " ".join([self.idx2word.get(idx, "<UNK>") for idx in token_ids])

    @property
    def vocab_size(self) -> int:
        return len(self.word2idx)
    
    @property
    def pad_id(self) -> int:
        return self.special_tokens["<PAD>"]

class NextTokenPredictionDataset(Dataset):
    """
    æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
    æ–‡ç« ã‚’å—ã‘å–ã‚Šã€(å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹, ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹)ã®ãƒšã‚¢ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    ä¾‹: "i love this movie" -> input="<START> i love this", target="i love this <END>"
    """
    def __init__(self, data: List[str], vocab: Vocabulary, max_len: int = 32):
        self.vocab = vocab
        self.max_len = max_len
        self.encoded_data = [self.vocab.encode(text) for text in data]
    
    def __len__(self):
        return len(self.encoded_data)
    
    def __getitem__(self, idx):
        encoded = self.encoded_data[idx]
        
        # å…¥åŠ›ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä½œæˆ
        input_seq = encoded[:-1]
        target_seq = encoded[1:]
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        input_len = len(input_seq)
        pad_len = self.max_len - input_len
        
        padded_input = input_seq[:self.max_len] + [self.vocab.pad_id] * max(0, pad_len)
        padded_target = target_seq[:self.max_len] + [self.vocab.pad_id] * max(0, pad_len)
        
        return torch.tensor(padded_input), torch.tensor(padded_target, dtype=torch.long)

# ----------------------------------------
# 2. æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
# ----------------------------------------

class SNNInferenceEngine:
    """SNNãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚„åˆ†æã‚’è¡Œã†æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³"""
    def __init__(self, model_path: str, device: str = "cpu"):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        
        self.device = torch.device(device)
        checkpoint = torch.load(model_path, map_location=self.device)
        
        self.vocab = checkpoint['vocab']
        config = checkpoint['config']
        
        self.model = BreakthroughSNN(vocab_size=self.vocab.vocab_size, **config).to(self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
    def generate(self, start_text: str, max_len: int = 20) -> str:
        """ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«ç¶šãæ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹"""
        print(f"\nç”Ÿæˆé–‹å§‹: '{start_text}'")
        
        # åˆæœŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        input_ids = self.vocab.encode(start_text, add_start_end=False)
        input_tensor = torch.tensor([input_ids], device=self.device)
        
        generated_ids = list(input_ids)
        
        with torch.no_grad():
            for _ in range(max_len):
                # ãƒ¢ãƒ‡ãƒ«ã«ç¾åœ¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å…¥åŠ›
                logits = self.model(input_tensor)
                
                # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬ç¢ºç‡ã‹ã‚‰æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                next_token_logits = logits[:, -1, :]
                next_token_id = torch.argmax(next_token_logits, dim=-1).item()
                
                # ç”ŸæˆãŒçµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã«é”ã—ãŸã‚‰çµ‚äº†
                if next_token_id == self.vocab.special_tokens["<END>"]:
                    break
                
                generated_ids.append(next_token_id)
                # æ¬¡ã®å…¥åŠ›ã¨ã—ã¦ã€ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
                input_tensor = torch.tensor([generated_ids], device=self.device)
        
        return self.vocab.decode(generated_ids)

# ----------------------------------------
# 3. å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯
# ----------------------------------------

def train():
    """ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®Ÿè¡Œ"""
    print("ğŸš€ é©æ–°çš„SNNã‚·ã‚¹ãƒ†ãƒ ã®è¨“ç·´é–‹å§‹ (æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã‚¿ã‚¹ã‚¯)")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤šæ§˜ãªæ–‡æ§‹é€ ï¼‰
    TRAIN_DATA = [
        "this movie was terrible", "i absolutely loved it",
        "a complete disappointment", "one of the best films ever made",
        "the plot was confusing and slow", "a truly heartwarming story",
        "i would not recommend this to anyone", "an unforgettable experience for sure",
        "what a complete mess", "simply fantastic from start to finish"
    ]
    
    vocab = Vocabulary(TRAIN_DATA)
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    config = {'d_model': 64, 'd_state': 32, 'num_layers': 2, 'time_steps': 16}
    model = BreakthroughSNN(vocab_size=vocab.vocab_size, **config)
    trainer = BreakthroughTrainer(model)
    
    dataset = NextTokenPredictionDataset(TRAIN_DATA, vocab)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    num_epochs = 100
    for epoch in range(num_epochs):
        for input_ids, target_ids in dataloader:
            metrics = trainer.train_step(input_ids, target_ids)
        if (epoch + 1) % 20 == 0:
            print(f"Epoch {epoch+1}/{num_epochs}: {metrics}")
            
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_path = "breakthrough_snn_model.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'vocab': vocab,
        'config': config
    }, model_path)
    print(f"\nâœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ '{model_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

def inference():
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ï¼ˆæ–‡ç« ç”Ÿæˆï¼‰ã‚’å®Ÿè¡Œ"""
    MODEL_FILE_PATH = "breakthrough_snn_model.pth"
    
    try:
        engine = SNNInferenceEngine(model_path=MODEL_FILE_PATH)
        test_sentences = [
            "this movie was",
            "i loved",
            "the story",
        ]
        
        for sentence in test_sentences:
            generated_text = engine.generate(sentence)
            print(f"ç”Ÿæˆçµæœ: {generated_text}")

    except FileNotFoundError as e:
        print(e)
        print("ã‚¨ãƒ©ãƒ¼: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚å…ˆã« 'train' ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "train":
        train()
    elif len(sys.argv) > 1 and sys.argv[1] == "inference":
        inference()
    else:
        print("ä½¿ã„æ–¹: python main.py [train|inference]")
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å­¦ç¿’ã‚’å®Ÿè¡Œ
        print("\n--- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ ---")
        train()
