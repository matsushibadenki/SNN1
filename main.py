# /path/to/your/project/main.py
# SNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (ãƒ‡ãƒ¼ã‚¿å½¢å¼ä»•æ§˜æ›¸v1.0å¯¾å¿œç‰ˆ)
#
# æ”¹å–„ç‚¹:
# - ãƒ‡ãƒ¼ã‚¿å½¢å¼ä»•æ§˜æ›¸ã«åŸºã¥ãã€.jsonlå½¢å¼ã®èª­ã¿è¾¼ã¿ã«å¯¾å¿œã€‚
# - --data_formatå¼•æ•°ã‚’å°å…¥ã—ã€'simple_text', 'dialogue', 'instruction'ã®å½¢å¼ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆå¯èƒ½ã«ã€‚
# - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆéƒ¨åˆ†ã‚’æŠ½è±¡åŒ–ã—ã€å„å½¢å¼ã«å¯¾å¿œã™ã‚‹å°‚ç”¨ã®Datasetã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…ã€‚
# - èªå½™æ§‹ç¯‰ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ±ç”¨åŒ–ã—ã€è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‹ã‚‰ã‚‚ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã‚‹ã‚ˆã†ã«æ”¹å–„ã€‚
# - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ã«å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã‚’é–¢æ•°åŒ–ã€‚
# - å­¦ç¿’ã®å®‰å®šåŒ–ã¨å†ç¾æ€§å‘ä¸Šã®ãŸã‚ã®æ©Ÿèƒ½ã‚’è¿½åŠ  (seed, scheduler, loss weights)ã€‚

import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from collections import Counter
import itertools
from typing import List, Tuple, Dict, Any, Iterator
import os
import random
import argparse
import json
from enum import Enum
import numpy as np

# snn_coreã‹ã‚‰ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from snn_core import BreakthroughSNN, BreakthroughTrainer, CombinedLoss

def set_seed(seed: int):
    """å­¦ç¿’ã®å†ç¾æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šã™ã‚‹ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    print(f"âœ… Random seed set to {seed}")

# ----------------------------------------
# 1. ãƒ‡ãƒ¼ã‚¿å½¢å¼ã¨ãƒ­ãƒ¼ãƒ€ãƒ¼
# ----------------------------------------
class DataFormat(Enum):
    """ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’å®šç¾©"""
    SIMPLE_TEXT = "simple_text"
    DIALOGUE = "dialogue"
    INSTRUCTION = "instruction"

def load_jsonl_data(file_path: str) -> Iterator[Dict[str, Any]]:
    """
    JSON Lines (.jsonl) ãƒ•ã‚¡ã‚¤ãƒ«ã‚’1è¡Œãšã¤é…å»¶èª­ã¿è¾¼ã¿ã—ã¾ã™ã€‚
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")

    if not file_path.endswith('.jsonl'):
        print(f"è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« '{file_path}' ã¯ .jsonl æ‹¡å¼µå­ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚JSON Lineså½¢å¼ã¨ã—ã¦å‡¦ç†ã‚’è©¦ã¿ã¾ã™ã€‚")

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                yield json.loads(line)

# ----------------------------------------
# 2. èªå½™ã®æ§‹ç¯‰
# ----------------------------------------
class Vocabulary:
    """ãƒ†ã‚­ã‚¹ãƒˆã¨IDã‚’ç›¸äº’å¤‰æ›ã™ã‚‹ãŸã‚ã®èªå½™ã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        # äºˆç´„ãƒˆãƒ¼ã‚¯ãƒ³
        self.special_tokens = {"<PAD>": 0, "<UNK>": 1, "<START>": 2, "<END>": 3}
        self.word2idx = self.special_tokens.copy()
        self.idx2word = {v: k for k, v in self.word2idx.items()}

    def build_vocab(self, all_texts: Iterator[str]):
        all_words = itertools.chain.from_iterable(txt.lower().split() for txt in all_texts)
        word_counts = Counter(all_words)
        for word, _ in word_counts.items():
            if word not in self.word2idx:
                idx = len(self.word2idx)
                self.word2idx[word] = idx
                self.idx2word[idx] = word
    
    def encode(self, text: str, add_start_end: bool = True) -> List[int]:
        tokens = [self.word2idx.get(word.lower(), self.special_tokens["<UNK>"]) for word in text.split()]
        if add_start_end:
            return [self.special_tokens["<START>"]] + tokens + [self.special_tokens["<END>"]]
        return tokens

    def decode(self, token_ids: List[int]) -> str:
        # <START> ã¨ <END> ãƒˆãƒ¼ã‚¯ãƒ³ã¯ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœã‹ã‚‰é™¤å¤–ã™ã‚‹ã“ã¨ãŒå¤šã„
        ids_to_decode = [idx for idx in token_ids if idx not in (self.special_tokens["<START>"], self.special_tokens["<END>"])]
        return " ".join([self.idx2word.get(idx, "<UNK>") for idx in ids_to_decode])

    @property
    def vocab_size(self) -> int:
        return len(self.word2idx)
    
    @property
    def pad_id(self) -> int:
        return self.special_tokens["<PAD>"]
# ----------------------------------------
# 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹
# ----------------------------------------
class SNNBaseDataset(Dataset):
    """å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    def __init__(self, file_path: str, vocab: Vocabulary):
        self.vocab = vocab
        self.data = list(load_jsonl_data(file_path))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:
        raise NotImplementedError

    @staticmethod
    def extract_texts(file_path: str) -> Iterator[str]:
        raise NotImplementedError

class SimpleTextDataset(SNNBaseDataset):
    """ 'simple_text' å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ """
    def __getitem__(self, idx):
        item = self.data[idx]
        encoded = self.vocab.encode(item['text'])
        return torch.tensor(encoded[:-1]), torch.tensor(encoded[1:], dtype=torch.long)

    @staticmethod
    def extract_texts(file_path: str) -> Iterator[str]:
        for item in load_jsonl_data(file_path):
            yield item['text']

class DialogueDataset(SNNBaseDataset):
    """ 'dialogue' å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ """
    def __getitem__(self, idx):
        item = self.data[idx]
        full_conversation = " ".join([turn['value'] for turn in item['conversations']])
        encoded = self.vocab.encode(full_conversation)
        return torch.tensor(encoded[:-1]), torch.tensor(encoded[1:], dtype=torch.long)

    @staticmethod
    def extract_texts(file_path: str) -> Iterator[str]:
        for item in load_jsonl_data(file_path):
            for turn in item['conversations']:
                yield turn['value']

class InstructionDataset(SNNBaseDataset):
    """ 'instruction' å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ """
    def __getitem__(self, idx):
        item = self.data[idx]
        prompt = item['instruction']
        if 'input' in item and item['input']:
            prompt += f"\n{item['input']}"
        full_text = f"{prompt}\n{item['output']}"
        encoded = self.vocab.encode(full_text)
        return torch.tensor(encoded[:-1]), torch.tensor(encoded[1:], dtype=torch.long)

    @staticmethod
    def extract_texts(file_path: str) -> Iterator[str]:
        for item in load_jsonl_data(file_path):
            yield item['instruction']
            if 'input' in item and item['input']:
                yield item['input']
            yield item['output']

def create_dataset(data_format: DataFormat, file_path: str, vocab: Vocabulary) -> SNNBaseDataset:
    format_map = {
        DataFormat.SIMPLE_TEXT: SimpleTextDataset,
        DataFormat.DIALOGUE: DialogueDataset,
        DataFormat.INSTRUCTION: InstructionDataset
    }
    data_format_enum = DataFormat(data_format) if isinstance(data_format, str) else data_format
    if data_format_enum not in format_map:
        raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ã™: {data_format}")
    return format_map[data_format_enum](file_path, vocab)

def get_text_extractor(data_format: DataFormat) -> callable:
    format_map = {
        DataFormat.SIMPLE_TEXT: SimpleTextDataset.extract_texts,
        DataFormat.DIALOGUE: DialogueDataset.extract_texts,
        DataFormat.INSTRUCTION: InstructionDataset.extract_texts
    }
    data_format_enum = DataFormat(data_format) if isinstance(data_format, str) else data_format
    return format_map[data_format_enum]

def collate_fn(batch: List[Tuple[torch.Tensor, torch.Tensor]], pad_id: int) -> Tuple[torch.Tensor, torch.Tensor]:
    inputs, targets = zip(*batch)
    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=pad_id)
    padded_targets = pad_sequence(targets, batch_first=True, padding_value=pad_id)
    return padded_inputs, padded_targets

# ----------------------------------------
# 4. æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
# ----------------------------------------
class SNNInferenceEngine:
    """SNNãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚„åˆ†æã‚’è¡Œã†æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³"""
    def __init__(self, model_path: str, device: str = "cpu"):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        
        self.device = torch.device(device)
        checkpoint = torch.load(model_path, map_location=self.device)
        
        self.vocab = checkpoint['vocab']
        config = checkpoint['config']
        
        self.model = BreakthroughSNN(vocab_size=self.vocab.vocab_size, **config).to(self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
    def generate(self, start_text: str, max_len: int = 20) -> str:
        input_ids = self.vocab.encode(start_text, add_start_end=True)[:-1]
        input_tensor = torch.tensor([input_ids], device=self.device)
        
        generated_ids = list(input_ids)
        
        with torch.no_grad():
            for _ in range(max_len):
                logits, _ = self.model(input_tensor, return_spikes=True)
                next_token_logits = logits[:, -1, :]
                next_token_id = torch.argmax(next_token_logits, dim=-1).item()
                
                if next_token_id == self.vocab.special_tokens["<END>"]:
                    break
                
                generated_ids.append(next_token_id)
                input_tensor = torch.cat([input_tensor, torch.tensor([[next_token_id]], device=self.device)], dim=1)
        
        return self.vocab.decode(generated_ids)

# ----------------------------------------
# 5. å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯
# ----------------------------------------
def run_training(args: argparse.Namespace, vocab: Vocabulary = None) -> Vocabulary:
    """
    ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®Ÿè¡Œã—ã€å­¦ç¿’æ¸ˆã¿ã®èªå½™ã‚’è¿”ã™ã€‚
    å¤–éƒ¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãªã©ï¼‰ã‹ã‚‰ã®å‘¼ã³å‡ºã—ã‚’æƒ³å®šã€‚
    """
    print(f"ğŸš€ é©æ–°çš„SNNã‚·ã‚¹ãƒ†ãƒ ã®è¨“ç·´é–‹å§‹ (ãƒ‡ãƒ¼ã‚¿å½¢å¼: {args.data_format})")
    
    try:
        # èªå½™ãŒæä¾›ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ–°è¦ã«æ§‹ç¯‰
        if vocab is None:
            vocab = Vocabulary()
            print("ğŸ“– èªå½™ã‚’æ§‹ç¯‰ä¸­...")
            text_extractor = get_text_extractor(args.data_format)
            vocab.build_vocab(text_extractor(args.data_path))
            print(f"âœ… èªå½™ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚èªå½™æ•°: {vocab.vocab_size}")

        dataset = create_dataset(args.data_format, args.data_path, vocab)
        custom_collate_fn = lambda batch: collate_fn(batch, vocab.pad_id)
        dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, collate_fn=custom_collate_fn, num_workers=2)
        print(f"âœ… {args.data_path} ã‹ã‚‰ {len(dataset)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

    except (FileNotFoundError, KeyError, TypeError, ValueError, json.JSONDecodeError) as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\nè©³ç´°: {e}")
        print("ãƒ’ãƒ³ãƒˆ: --data_format å¼•æ•°ãŒãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã€.jsonl ãƒ•ã‚¡ã‚¤ãƒ«ãŒä»•æ§˜æ›¸é€šã‚Šã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        raise e

    config = {'d_model': args.d_model, 'd_state': args.d_state, 'num_layers': args.num_layers, 'time_steps': args.time_steps}
    model = BreakthroughSNN(vocab_size=vocab.vocab_size, **config)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate)
    
    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    scheduler = None
    if args.use_scheduler:
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=1e-6)
        print("âœ… CosineAnnealingLRã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’æœ‰åŠ¹ã«ã—ã¾ã—ãŸã€‚")

    # æå¤±é–¢æ•°ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆé‡ã¿ã‚’å¼•æ•°ã‹ã‚‰è¨­å®šï¼‰
    criterion = CombinedLoss(
        ce_weight=args.ce_weight,
        spike_reg_weight=args.spike_reg_weight,
        pad_id=vocab.pad_id
    )
    
    trainer = BreakthroughTrainer(model, optimizer, criterion, scheduler=scheduler)
    
    print("\nğŸ”¥ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    for epoch in range(args.epochs):
        train_metrics = trainer.train_epoch(dataloader)
        # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒãªã„ãŸã‚ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ä»£ç”¨ï¼ˆéå­¦ç¿’ã®ãƒªã‚¹ã‚¯ã‚ã‚Šï¼‰
        val_metrics = trainer.evaluate(dataloader)
        if (epoch + 1) % args.log_interval == 0:
            lr = scheduler.get_last_lr()[0] if scheduler else args.learning_rate
            metrics_str = ", ".join([f"train_{k}: {v:.4f}" for k, v in train_metrics.items()])
            metrics_str += ", " + ", ".join([f"val_{k}: {v:.4f}" for k, v in val_metrics.items()])
            print(f"Epoch {epoch+1: >3}/{args.epochs}: {metrics_str}, lr: {lr:.6f}")
            
    model_path = args.model_path
    torch.save({
        'model_state_dict': model.state_dict(),
        'vocab': vocab,
        'config': config
    }, model_path)
    print(f"\nâœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ '{model_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    return vocab

def start_inference_cli(args):
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ï¼ˆæ–‡ç« ç”Ÿæˆï¼‰ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®CLI"""
    try:
        engine = SNNInferenceEngine(model_path=args.model_path)
        
        print("\nğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¾ãŸã¯ 'quit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        while True:
            start_text = input("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: ")
            if start_text.lower() in ["exit", "quit"]:
                break
            generated_text = engine.generate(start_text, max_len=args.max_len)
            print(f"ç”Ÿæˆçµæœ: {generated_text}")

    except FileNotFoundError as e:
        print(f"âŒ {e}")
        print(f"ã‚¨ãƒ©ãƒ¼: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«({args.model_path})ãŒå¿…è¦ã§ã™ã€‚å…ˆã« 'train' ã‚³ãƒãƒ³ãƒ‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        print(f"âŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SNNãƒ™ãƒ¼ã‚¹ AIãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ  (ãƒ‡ãƒ¼ã‚¿å½¢å¼ä»•æ§˜æ›¸v1.0å¯¾å¿œ)")
    subparsers = parser.add_subparsers(dest="command", required=True, help="å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒ‰")

    # --- å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰ ---
    parser_train = subparsers.add_parser("train", help="SNNãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã™")
    parser_train.add_argument("data_path", type=str, help="å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.jsonl)")
    parser_train.add_argument(
        "--data_format",
        type=DataFormat,
        default=DataFormat.SIMPLE_TEXT,
        choices=list(DataFormat),
        help="å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼"
    )
    parser_train.add_argument("--epochs", type=int, default=10, help="å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°")
    parser_train.add_argument("--batch_size", type=int, default=16, help="ãƒãƒƒãƒã‚µã‚¤ã‚º")
    parser_train.add_argument("--learning_rate", type=float, default=5e-4, help="å­¦ç¿’ç‡")
    parser_train.add_argument("--log_interval", type=int, default=1, help="ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‚¨ãƒãƒƒã‚¯é–“éš”")
    parser_train.add_argument("--model_path", type=str, default="breakthrough_snn_model.pth", help="å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹")
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    parser_train.add_argument("--d_model", type=int, default=64)
    parser_train.add_argument("--d_state", type=int, default=32)
    parser_train.add_argument("--num_layers", type=int, default=2)
    parser_train.add_argument("--time_steps", type=int, default=16)
    # å­¦ç¿’å®‰å®šåŒ–ã®ãŸã‚ã®å¼•æ•°ã‚’è¿½åŠ 
    parser_train.add_argument("--seed", type=int, default=42, help="ä¹±æ•°ã‚·ãƒ¼ãƒ‰")
    parser_train.add_argument("--ce_weight", type=float, default=1.0, help="ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã®é‡ã¿")
    parser_train.add_argument("--spike_reg_weight", type=float, default=0.01, help="ã‚¹ãƒ‘ã‚¤ã‚¯æ­£å‰‡åŒ–æå¤±ã®é‡ã¿")
    parser_train.add_argument("--use_scheduler", action='store_true', help="å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    
    # --- æ¨è«–ã‚³ãƒãƒ³ãƒ‰ ---
    parser_inference = subparsers.add_parser("inference", help="å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ï¼ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼‰ã‚’å®Ÿè¡Œã—ã¾ã™")
    parser_inference.add_argument("--model_path", type=str, default="breakthrough_snn_model.pth", help="å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹")
    parser_inference.add_argument("--max_len", type=int, default=40, help="ç”Ÿæˆã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å¤§é•·")

    args = parser.parse_args()

    if args.command == "train":
        set_seed(args.seed)
        run_training(args)
    elif args.command == "inference":
        start_inference_cli(args)
