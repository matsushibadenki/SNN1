# /path/to/your/project/snn_neuromorphic_optimization.py
# ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å‘ã‘é«˜åº¦æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
# 
# æœ€æ–°ç ”ç©¶ã«åŸºã¥ãå®Ÿè£…:
# - Event-driven sparse computationæœ€é©åŒ–
# - ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒãƒƒãƒ—ç‰¹æ€§æ´»ç”¨
# - é‡å­åŒ–ãƒ»ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã®é«˜åº¦åŒ–
# - ãƒ¡ãƒ¢ãƒªéšå±¤æœ€é©åŒ–
# - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ä¿è¨¼

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, Tuple, Optional, List, Union
import time
import threading
from queue import Queue
from dataclasses import dataclass
from enum import Enum
import math

# ----------------------------------------
# 1. ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
# ----------------------------------------

class NeuromorphicChip(Enum):
    """ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒãƒƒãƒ—ã‚¿ã‚¤ãƒ—"""
    INTEL_LOIHI = "intel_loihi"
    IBM_TRUENORTH = "ibm_truenorth"
    SPINNAKER = "spinnaker"
    BRAINDROP = "braindrop"
    GENERIC_EDGE = "generic_edge"

@dataclass
class NeuromorphicProfile:
    """ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
    chip_type: NeuromorphicChip
    num_cores: int
    neurons_per_core: int
    synapses_per_core: int
    memory_hierarchy: Dict[str, int]  # {"L1": 64KB, "L2": 512KB, "DRAM": 8GB}
    event_throughput: int  # events/second
    power_budget_mw: float
    supports_online_learning: bool = True
    supports_stdp: bool = True

# ----------------------------------------
# 2. Event-Drivenç–è¡Œåˆ—æœ€é©åŒ–
# ----------------------------------------

class SparseEventMatrix:
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯å‘ã‘Event-Drivenç–è¡Œåˆ—
    ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æœ€é©åŒ–
    """
    def __init__(self, rows: int, cols: int, sparsity: float = 0.9):
        self.rows = rows
        self.cols = cols
        self.sparsity = sparsity
        
        # COO (Coordinate) format for neuromorphic efficiency
        self.nnz = int(rows * cols * (1 - sparsity))
        self.row_indices = torch.randint(0, rows, (self.nnz,), dtype=torch.int32)
        self.col_indices = torch.randint(0, cols, (self.nnz,), dtype=torch.int32)
        self.values = torch.randn(self.nnz)
        
        # Pre-compute access patterns for neuromorphic cores
        self._build_core_mapping()
        
    def _build_core_mapping(self):
        """ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ã‚³ã‚¢é–“ã§ã®ãƒãƒƒãƒ”ãƒ³ã‚°æœ€é©åŒ–"""
        # Core assignment based on row blocks
        self.core_assignments = self.row_indices // (self.rows // 16)  # 16 cores
        
        # Sort by core for sequential access
        sort_idx = torch.argsort(self.core_assignments)
        self.row_indices = self.row_indices[sort_idx]
        self.col_indices = self.col_indices[sort_idx]
        self.values = self.values[sort_idx]
        self.core_assignments = self.core_assignments[sort_idx]
    
    def event_driven_multiply(self, spike_vector: torch.Tensor, 
                            active_threshold: float = 0.01) -> torch.Tensor:
        """Event-drivenãªç–è¡Œåˆ—ä¹—ç®—"""
        # Only process non-zero spikes (event-driven)
        active_indices = (spike_vector > active_threshold).nonzero(as_tuple=True)[0]
        
        if len(active_indices) == 0:
            return torch.zeros(self.rows)
        
        # Filter relevant matrix elements
        mask = torch.isin(self.col_indices, active_indices)
        active_rows = self.row_indices[mask]
        active_cols = self.col_indices[mask]
        active_values = self.values[mask]
        
        # Compute result only for active connections
        result = torch.zeros(self.rows)
        for i, (r, c, v) in enumerate(zip(active_rows, active_cols, active_values)):
            result[r] += v * spike_vector[c]
            
        return result

class NeuromorphicMemoryManager:
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®ãƒ¡ãƒ¢ãƒªéšå±¤æœ€é©åŒ–
    """
    def __init__(self, profile: NeuromorphicProfile):
        self.profile = profile
        self.memory_pools = {}
        self.access_patterns = {}
        self.cache_stats = {"hits": 0, "misses": 0}
        
        # Initialize memory pools
        for level, size in profile.memory_hierarchy.items():
            self.memory_pools[level] = {}
            
    def allocate_weights(self, layer_name: str, weight_tensor: torch.Tensor) -> str:
        """é‡ã¿ãƒ†ãƒ³ã‚½ãƒ«ã®æœ€é©ãƒ¡ãƒ¢ãƒªé…ç½®"""
        tensor_size = weight_tensor.numel() * weight_tensor.element_size()
        
        # Memory hierarchy traversal (L1 -> L2 -> DRAM)
        for level in ["L1", "L2", "DRAM"]:
            if level in self.profile.memory_hierarchy:
                available_space = self.profile.memory_hierarchy[level] - sum(
                    t.numel() * t.element_size() for t in self.memory_pools[level].values()
                )
                
                if tensor_size <= available_space:
                    # Quantize weights based on memory level
                    if level == "L1":
                        # Highest precision for L1 cache
                        quantized_weights = weight_tensor
                    elif level == "L2":
                        # Medium precision for L2
                        quantized_weights = torch.quantize_per_tensor(
                            weight_tensor, scale=0.1, zero_point=0, dtype=torch.qint8
                        )
                    else:  # DRAM
                        # Lower precision for DRAM
                        quantized_weights = torch.quantize_per_tensor(
                            weight_tensor, scale=0.2, zero_point=0, dtype=torch.qint8
                        )
                    
                    self.memory_pools[level][layer_name] = quantized_weights
                    return level
        
        raise RuntimeError(f"Cannot allocate {tensor_size} bytes for {layer_name}")
    
    def prefetch_weights(self, layer_names: List[str]):
        """æ¬¡ã®è¨ˆç®—å±¤ã®é‡ã¿ã‚’å…ˆè¡Œèª­ã¿è¾¼ã¿"""
        for layer_name in layer_names:
            # Move frequently accessed weights to L1
            if layer_name in self.access_patterns:
                self.access_patterns[layer_name] += 1
                if self.access_patterns[layer_name] > 10:  # Hot data
                    self._promote_to_l1(layer_name)
            else:
                self.access_patterns[layer_name] = 1
                
    def _promote_to_l1(self, layer_name: str):
        """é‡è¦ãªé‡ã¿ã‚’L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æ˜‡æ ¼"""
        for level in ["L2", "DRAM"]:
            if level in self.memory_pools and layer_name in self.memory_pools[level]:
                weights = self.memory_pools[level][layer_name]
                try:
                    new_level = self.allocate_weights(f"{layer_name}_promoted", weights)
                    if new_level == "L1":
                        del self.memory_pools[level][layer_name]
                        break
                except RuntimeError:
                    break

# ----------------------------------------
# 3. Real-Time Event Processing
# ----------------------------------------

class RealtimeEventProcessor:
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
    ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã¨ã‚¿ã‚¤ãƒ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å‡¦ç†ã‚’ä¿è¨¼
    """
    def __init__(self, max_latency_ms: float = 1.0, 
                 event_buffer_size: int = 10000):
        self.max_latency_ms = max_latency_ms
        self.event_buffer_size = event_buffer_size
        
        # Event queues with priority
        self.high_priority_queue = Queue(maxsize=event_buffer_size)
        self.low_priority_queue = Queue(maxsize=event_buffer_size)
        
        # Performance metrics
        self.processing_times = []
        self.dropped_events = 0
        self.processed_events = 0
        
        # Real-time constraints
        self.deadline_misses = 0
        self.is_processing = False
        
    def submit_spike_event(self, timestamp: float, neuron_id: int, 
                          spike_value: float, priority: str = "normal"):
        """ã‚¹ãƒ‘ã‚¤ã‚¯ã‚¤ãƒ™ãƒ³ãƒˆã®æå‡º"""
        event = {
            'timestamp': timestamp,
            'neuron_id': neuron_id,
            'spike_value': spike_value,
            'submitted_at': time.time() * 1000  # ms
        }
        
        try:
            if priority == "high":
                self.high_priority_queue.put_nowait(event)
            else:
                self.low_priority_queue.put_nowait(event)
        except:
            self.dropped_events += 1
            
    def process_events_batch(self, model_layer, max_events: int = 1000) -> torch.Tensor:
        """ãƒãƒƒãƒã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ¶ç´„ä»˜ãï¼‰"""
        start_time = time.time() * 1000  # ms
        events = []
        
        # High priority events first
        while not self.high_priority_queue.empty() and len(events) < max_events:
            events.append(self.high_priority_queue.get_nowait())
            
        # Fill remaining with low priority
        while not self.low_priority_queue.empty() and len(events) < max_events:
            events.append(self.low_priority_queue.get_nowait())
        
        if not events:
            return torch.zeros(1)
        
        # Convert events to tensor format
        spike_tensor = self._events_to_tensor(events)
        
        # Process with model layer
        self.is_processing = True
        try:
            result = model_layer(spike_tensor)
        finally:
            self.is_processing = False
            
        # Check real-time constraints
        processing_time = time.time() * 1000 - start_time
        self.processing_times.append(processing_time)
        
        if processing_time > self.max_latency_ms:
            self.deadline_misses += 1
            
        self.processed_events += len(events)
        return result
        
    def _events_to_tensor(self, events: List[Dict]) -> torch.Tensor:
        """ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ã«å¤‰æ›"""
        if not events:
            return torch.zeros(1, 1)
            
        max_neuron_id = max(e['neuron_id'] for e in events)
        spike_tensor = torch.zeros(1, max_neuron_id + 1)
        
        for event in events:
            spike_tensor[0, event['neuron_id']] = event['spike_value']
            
        return spike_tensor
    
    def get_performance_stats(self) -> Dict[str, float]:
        """æ€§èƒ½çµ±è¨ˆã®å–å¾—"""
        if not self.processing_times:
            return {}
            
        return {
            'avg_latency_ms': np.mean(self.processing_times),
            'max_latency_ms': np.max(self.processing_times),
            'deadline_miss_rate': self.deadline_misses / max(1, self.processed_events),
            'throughput_events_sec': self.processed_events / (np.sum(self.processing_times) / 1000),
            'dropped_event_rate': self.dropped_events / max(1, self.dropped_events + self.processed_events)
        }

# ----------------------------------------
# 4. é©å¿œçš„é‡å­åŒ–ãƒ»ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
# ----------------------------------------

class AdaptiveQuantizationPruning:
    """
    å‹•çš„ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«å¿œã˜ãŸé©å¿œçš„é‡å­åŒ–ãƒ»ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°
    """
    def __init__(self, target_latency_ms: float = 10.0,
                 target_accuracy: float = 0.95,
                 min_sparsity: float = 0.5,
                 max_sparsity: float = 0.95):
        
        self.target_latency = target_latency_ms
        self.target_accuracy = target_accuracy
        self.min_sparsity = min_sparsity
        self.max_sparsity = max_sparsity
        
        # Adaptation parameters
        self.current_sparsity = min_sparsity
        self.current_bit_width = 8  # Start with int8
        self.performance_history = deque(maxlen=100)
        self.accuracy_history = deque(maxlen=100)
        
    def adapt_compression(self, current_latency: float, 
                         current_accuracy: float) -> Dict[str, Any]:
        """ç¾åœ¨ã®æ€§èƒ½ã«åŸºã¥ã„ã¦åœ§ç¸®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©å¿œ"""
        
        self.performance_history.append(current_latency)
        self.accuracy_history.append(current_accuracy)
        
        adaptation_needed = False
        new_config = {
            'sparsity': self.current_sparsity,
            'bit_width': self.current_bit_width
        }
        
        # Latency-based adaptation
        if current_latency > self.target_latency * 1.2:
            # Too slow - increase compression
            if self.current_sparsity < self.max_sparsity:
                new_config['sparsity'] = min(self.max_sparsity, 
                                           self.current_sparsity + 0.05)
                adaptation_needed = True
            elif self.current_bit_width > 4:
                new_config['bit_width'] = max(4, self.current_bit_width - 1)
                adaptation_needed = True
                
        elif current_latency < self.target_latency * 0.8:
            # Too fast - can reduce compression for better accuracy
            if current_accuracy < self.target_accuracy:
                if self.current_sparsity > self.min_sparsity:
                    new_config['sparsity'] = max(self.min_sparsity,
                                               self.current_sparsity - 0.05)
                    adaptation_needed = True
                elif self.current_bit_width < 16:
                    new_config['bit_width'] = min(16, self.current_bit_width + 1)
                    adaptation_needed = True
        
        # Update current state
        if adaptation_needed:
            self.current_sparsity = new_config['sparsity']
            self.current_bit_width = new_config['bit_width']
            
        return new_config if adaptation_needed else {}
    
    def apply_adaptive_pruning(self, model: nn.Module, 
                              sparsity: float) -> nn.Module:
        """é©å¿œçš„æ§‹é€ ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°"""
        import torch.nn.utils.prune as prune
        
        # Calculate layer-wise sparsity based on importance
        layer_importance = self._calculate_layer_importance(model)
        
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                # Adaptive sparsity based on layer importance
                layer_sparsity = sparsity * (2.0 - layer_importance.get(name, 1.0))
                layer_sparsity = max(0.1, min(0.95, layer_sparsity))
                
                # Apply magnitude-based pruning
                prune.l1_unstructured(module, name="weight", amount=layer_sparsity)
                prune.remove(module, 'weight')
                
        return model
    
    def _calculate_layer_importance(self, model: nn.Module) -> Dict[str, float]:
        """ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡è¦åº¦è¨ˆç®—"""
        importance_scores = {}
        
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                # Calculate importance based on weight magnitude distribution
                weight_norms = torch.norm(module.weight, dim=1)
                importance = float(weight_norms.std() / (weight_norms.mean() + 1e-8))
                importance_scores[name] = importance
                
        return importance_scores

# ----------------------------------------
# 5. ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
# ----------------------------------------

class NeuromorphicDeploymentManager:
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å‘ã‘çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    """
    def __init__(self, profile: NeuromorphicProfile):
        self.profile = profile
        self.memory_manager = NeuromorphicMemoryManager(profile)
        self.event_processor = RealtimeEventProcessor(max_latency_ms=5.0)
        self.adaptive_compression = AdaptiveQuantizationPruning()
        
        # Deployed models with neuromorphic optimization
        self.deployed_models = {}
        
    def deploy_neuromorphic_model(self, 
                                model: nn.Module,
                                model_name: str,
                                optimization_target: str = "balanced") -> Dict[str, Any]:
        """ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤"""
        
        print(f"ğŸ”§ ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé–‹å§‹: {model_name}")
        
        # Step 1: Hardware-aware model optimization
        optimized_model = self._optimize_for_neuromorphic(model, optimization_target)
        
        # Step 2: Memory allocation and weight quantization
        memory_layout = self._allocate_model_memory(optimized_model, model_name)
        
        # Step 3: Event-driven processing setup
        event_config = self._configure_event_processing(optimized_model)
        
        # Step 4: Real-time monitoring setup
        monitor = self._setup_performance_monitor(model_name)
        
        deployment_info = {
            'model': optimized_model,
            'memory_layout': memory_layout,
            'event_config': event_config,
            'monitor': monitor,
            'deployed_at': time.time()
        }
        
        self.deployed_models[model_name] = deployment_info
        
        print(f"âœ… ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Œäº†: {model_name}")
        return deployment_info
    
    def _optimize_for_neuromorphic(self, model: nn.Module, target: str) -> nn.Module:
        """ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å‘ã‘æœ€é©åŒ–"""
        
        if target == "ultra_low_power":
            # Aggressive optimization for power efficiency
            sparsity = 0.9
            bit_width = 4
        elif target == "balanced":
            sparsity = 0.7
            bit_width = 8
        else:  # high_performance
            sparsity = 0.5
            bit_width = 16
            
        # Apply adaptive pruning
        optimized_model = self.adaptive_compression.apply_adaptive_pruning(model, sparsity)
        
        # Convert to neuromorphic-friendly layers
        optimized_model = self._convert_to_sparse_layers(optimized_model)
        
        return optimized_model
    
    def _convert_to_sparse_layers(self, model: nn.Module) -> nn.Module:
        """æ¨™æº–å±¤ã‚’ç–è¡Œåˆ—å±¤ã«å¤‰æ›"""
        for name, module in model.named_children():
            if isinstance(module, nn.Linear):
                # Replace with sparse event-driven layer
                sparse_layer = SparseEventLinear(
                    module.in_features, 
                    module.out_features,
                    sparsity=0.8
                )
                sparse_layer.load_from_dense(module)
                setattr(model, name, sparse_layer)
            elif len(list(module.children())) > 0:
                self._convert_to_sparse_layers(module)
                
        return model
    
    def _allocate_model_memory(self, model: nn.Module, model_name: str) -> Dict[str, str]:
        """ãƒ¢ãƒ‡ãƒ«é‡ã¿ã®ãƒ¡ãƒ¢ãƒªéšå±¤ã¸ã®é…ç½®"""
        memory_layout = {}
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                memory_level = self.memory_manager.allocate_weights(
                    f"{model_name}_{name}", param.data
                )
                memory_layout[name] = memory_level
                
        return memory_layout
    
    def _configure_event_processing(self, model: nn.Module) -> Dict[str, Any]:
        """ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ã®è¨­å®š"""
        return {
            'max_events_per_batch': 5000,
            'priority_threshold': 0.1,
            'buffer_size': 10000
        }
    
    def _setup_performance_monitor(self, model_name: str):
        """æ€§èƒ½ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        return {
            'start_time': time.time(),
            'inference_count': 0,
            'total_latency': 0.0,
            'accuracy_samples': []
        }
    
    def neuromorphic_inference(self, 
                              model_name: str, 
                              spike_input: torch.Tensor,
                              real_time: bool = True) -> Tuple[torch.Tensor, Dict[str, float]]:
        """ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯æ¨è«–å®Ÿè¡Œ"""
        
        if model_name not in self.deployed_models:
            raise KeyError(f"Model {model_name} not deployed")
            
        deployment = self.deployed_models[model_name]
        model = deployment['model']
        monitor = deployment['monitor']
        
        start_time = time.time() * 1000  # ms
        
        try:
            if real_time:
                # Real-time event-driven processing
                result = self.event_processor.process_events_batch(model, max_events=1000)
            else:
                # Standard batch processing
                model.eval()
                with torch.no_grad():
                    result = model(spike_input)
                    
            # Update performance metrics
            latency = time.time() * 1000 - start_time
            monitor['inference_count'] += 1
            monitor['total_latency'] += latency
            
            # Adaptive optimization based on performance
            avg_latency = monitor['total_latency'] / monitor['inference_count']
            if monitor['inference_count'] % 100 == 0:  # Periodic adaptation
                adaptation = self.adaptive_compression.adapt_compression(avg_latency, 0.9)
                if adaptation:
                    print(f"ğŸ”„ é©å¿œçš„æœ€é©åŒ–é©ç”¨: {adaptation}")
            
            performance_stats = {
                'latency_ms': latency,
                'avg_latency_ms': avg_latency,
                'throughput_infer_sec': 1000.0 / latency if latency > 0 else 0
            }
            
            return result, performance_stats
            
        except Exception as e:
            print(f"âŒ æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return torch.zeros(1), {'error': str(e)}
    
    def get_deployment_status(self, model_name: str) -> Dict[str, Any]:
        """ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆçŠ¶æ…‹ã®å–å¾—"""
        if model_name not in self.deployed_models:
            return {'status': 'not_deployed'}
            
        deployment = self.deployed_models[model_name]
        monitor = deployment['monitor']
        
        # Memory usage analysis
        memory_usage = {}
        for level, pool in self.memory_manager.memory_pools.items():
            used = sum(t.numel() * t.element_size() for t in pool.values())
            total = self.profile.memory_hierarchy[level]
            memory_usage[level] = {'used': used, 'total': total, 'utilization': used / total}
        
        # Event processing stats
        event_stats = self.event_processor.get_performance_stats()
        
        return {
            'status': 'active',
            'model_name': model_name,
            'deployed_at': deployment['deployed_at'],
            'inference_count': monitor['inference_count'],
            'avg_latency_ms': monitor['total_latency'] / max(1, monitor['inference_count']),
            'memory_usage': memory_usage,
            'event_processing': event_stats,
            'hardware_profile': {
                'chip_type': self.profile.chip_type.value,
                'num_cores': self.profile.num_cores,
                'power_budget_mw': self.profile.power_budget_mw
            }
        }

# ----------------------------------------
# 6. ç–è¡Œåˆ—Event-drivenå±¤
# ----------------------------------------

class SparseEventLinear(nn.Module):
    """Event-drivenç–è¡Œåˆ—ç·šå½¢å±¤"""
    def __init__(self, in_features: int, out_features: int, sparsity: float = 0.8):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.sparsity = sparsity
        
        # Sparse matrix representation
        self.sparse_matrix = SparseEventMatrix(out_features, in_features, sparsity)
        self.bias = nn.Parameter(torch.zeros(out_features))
        
    def load_from_dense(self, dense_layer: nn.Linear):
        """å¯†è¡Œåˆ—å±¤ã‹ã‚‰ç–è¡Œåˆ—ã¸ã®å¤‰æ›"""
        dense_weights = dense_layer.weight.data
        
        # Convert to sparse representation
        threshold = torch.quantile(torch.abs(dense_weights), self.sparsity)
        mask = torch.abs(dense_weights) > threshold
        
        # Extract sparse elements
        sparse_indices = torch.nonzero(mask, as_tuple=False)
        self.sparse_matrix.row_indices = sparse_indices[:, 0].int()
        self.sparse_matrix.col_indices = sparse_indices[:, 1].int()
        self.sparse_matrix.values = dense_weights[mask]
        self.sparse_matrix.nnz = len(self.sparse_matrix.values)
        
        if dense_layer.bias is not None:
            self.bias.data = dense_layer.bias.data
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Event-driven sparse matrix multiplication
        batch_size = x.shape[0]
        results = []
        
        for i in range(batch_size):
            result = self.sparse_matrix.event_driven_multiply(x[i])
            results.append(result)
            
        output = torch.stack(results) + self.bias
        return output

# ----------------------------------------
# 7. ä½¿ç”¨ä¾‹ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
# ----------------------------------------

def benchmark_neuromorphic_deployment():
    """ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("ğŸš€ ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
    
    # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆIntel Loihié¢¨ï¼‰
    profile = NeuromorphicProfile(
        chip_type=NeuromorphicChip.INTEL_LOIHI,
        num_cores=128,
        neurons_per_core=1024,
        synapses_per_core=8192,
        memory_hierarchy={"L1": 65536, "L2": 524288, "DRAM": 8589934592},  # bytes
        event_throughput=1000000,  # 1M events/sec
        power_budget_mw=100.0
    )
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä½œæˆ
    test_model = nn.Sequential(
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.ReLU(),
        nn.Linear(64, 10)
    )
    
    # ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    manager = NeuromorphicDeploymentManager(profile)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤
    deployment_info = manager.deploy_neuromorphic_model(
        test_model, "test_neuromorphic_model", optimization_target="balanced"
    )
    
    print("ğŸ“Š ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæƒ…å ±:")
    print(f"  ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ: {deployment_info['memory_layout']}")
    
    # æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    test_input = torch.randn(8, 256)  # ãƒãƒƒãƒã‚µã‚¤ã‚º8
    
    print("\nğŸ”„ æ¨è«–æ€§èƒ½ãƒ†ã‚¹ãƒˆ...")
    latencies = []
    
    for i in range(50):
        result, stats = manager.neuromorphic_inference(
            "test_neuromorphic_model", test_input, real_time=True
        )
        latencies.append(stats['latency_ms'])
        
        if (i + 1) % 10 == 0:
            avg_latency = np.mean(latencies[-10:])
            print(f"  Batch {i+1}: å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ {avg_latency:.2f}ms")
    
    # æœ€çµ‚çµ±è¨ˆ
    final_stats = manager.get_deployment_status("test_neuromorphic_model")
    
    print(f"\nğŸ“ˆ æœ€çµ‚æ€§èƒ½çµ±è¨ˆ:")
    print(f"  ç·æ¨è«–å›æ•°: {final_stats['inference_count']}")
    print(f"  å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼: {final_stats['avg_latency_ms']:.2f}ms")
    print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡:")
    for level, usage in final_stats['memory_usage'].items():
        print(f"    {level}: {usage['utilization']*100:.1f}% ({usage['used']}/{usage['total']} bytes)")
    
    if 'event_processing' in final_stats:
        event_stats = final_stats['event_processing']
        if 'throughput_events_sec' in event_stats:
            print(f"  ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {event_stats['throughput_events_sec']:.0f} events/sec")
            print(f"  ãƒ‡ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ãƒŸã‚¹ç‡: {event_stats.get('deadline_miss_rate', 0)*100:.2f}%")
    
    print("âœ… ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ¼ãƒ•ã‚£ãƒƒã‚¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")

if __name__ == "__main__":
    benchmark_neuromorphic_deployment()