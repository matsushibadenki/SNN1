# /path/to/your/project/snn_advanced_plasticity.py
# ç”Ÿç‰©å­¦çš„ã‚·ãƒŠãƒ—ã‚¹å¯å¡‘æ€§ã‚’çµ±åˆã—ãŸæ¬¡ä¸–ä»£SNNã‚·ã‚¹ãƒ†ãƒ 
# 
# æœ€æ–°ç ”ç©¶ã«åŸºã¥ãå®Ÿè£…:
# - STDP (Spike-Timing-Dependent Plasticity) 
# - çŸ­æœŸã‚·ãƒŠãƒ—ã‚¹å¯å¡‘æ€§ (STP)
# - ãƒ¡ã‚¿å¯å¡‘æ€§ (Metaplasticity)
# - ãƒ›ãƒ¡ã‚ªã‚¹ã‚¿ã‚·ã‚¹æ©Ÿæ§‹
# - ãƒªã‚¶ãƒãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°çš„è‡ªå·±çµ„ç¹”åŒ–

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, Tuple, Optional, List
from spikingjelly.activation_based import neuron, surrogate, functional
import math
from collections import deque

# ----------------------------------------
# 1. STDPå¯å¡‘æ€§ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ----------------------------------------

class STDPSynapse(nn.Module):
    """
    Spike-Timing-Dependent Plasticity ã‚·ãƒŠãƒ—ã‚¹
    2024å¹´ã®æœ€æ–°ç ”ç©¶ã«åŸºã¥ãé«˜åŠ¹ç‡STDPå®Ÿè£…
    """
    def __init__(self, 
                 in_features: int, 
                 out_features: int,
                 tau_pre: float = 20.0,      # ãƒ—ãƒ¬ã‚·ãƒŠãƒ—ã‚¹ã®æ™‚å®šæ•°
                 tau_post: float = 20.0,     # ãƒã‚¹ãƒˆã‚·ãƒŠãƒ—ã‚¹ã®æ™‚å®šæ•°
                 A_pos: float = 0.01,        # LTPå¼·åº¦
                 A_neg: float = 0.005,       # LTDå¼·åº¦
                 w_min: float = 0.0,         # æœ€å°é‡ã¿
                 w_max: float = 1.0,         # æœ€å¤§é‡ã¿
                 homeostatic_scaling: bool = True):
        super().__init__()
        
        self.in_features = in_features
        self.out_features = out_features
        self.tau_pre = tau_pre
        self.tau_post = tau_post
        self.A_pos = A_pos
        self.A_neg = A_neg
        self.w_min = w_min
        self.w_max = w_max
        self.homeostatic_scaling = homeostatic_scaling
        
        # ã‚·ãƒŠãƒ—ã‚¹é‡ã¿
        self.weight = nn.Parameter(torch.rand(out_features, in_features) * 0.5 + 0.25)
        
        # STDPãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆãƒ—ãƒ¬ãƒ»ãƒã‚¹ãƒˆã‚·ãƒŠãƒ—ã‚¹ã®æ´»å‹•å±¥æ­´ï¼‰
        self.register_buffer('pre_trace', torch.zeros(1, in_features))
        self.register_buffer('post_trace', torch.zeros(1, out_features))
        
        # ãƒ›ãƒ¡ã‚ªã‚¹ã‚¿ã‚·ã‚¹ç”¨ã®æ´»å‹•ãƒ¬ãƒ¼ãƒˆè¿½è·¡
        if homeostatic_scaling:
            self.register_buffer('pre_rate', torch.ones(in_features) * 0.02)
            self.register_buffer('post_rate', torch.ones(out_features) * 0.02)
            self.target_rate = 0.02  # ç›®æ¨™ç™ºç«ç‡
            self.homeostatic_alpha = 0.001
        
        # æ¸›è¡°ä¿‚æ•°ï¼ˆäº‹å‰è¨ˆç®—ã§åŠ¹ç‡åŒ–ï¼‰
        self.pre_decay = math.exp(-1.0 / tau_pre)
        self.post_decay = math.exp(-1.0 / tau_post)
        
    def forward(self, pre_spike: torch.Tensor, post_spike: torch.Tensor, 
                learning: bool = True) -> torch.Tensor:
        """
        Args:
            pre_spike: ãƒ—ãƒ¬ã‚·ãƒŠãƒ—ã‚¹ã‚¹ãƒ‘ã‚¤ã‚¯ (batch_size, in_features)
            post_spike: ãƒã‚¹ãƒˆã‚·ãƒŠãƒ—ã‚¹ã‚¹ãƒ‘ã‚¤ã‚¯ (batch_size, out_features)
            learning: STDPå­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹
        Returns:
            æ›´æ–°ã•ã‚ŒãŸã‚·ãƒŠãƒ—ã‚¹å‡ºåŠ›
        """
        batch_size = pre_spike.shape[0]
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’èª¿æ•´
        if self.pre_trace.shape[0] != batch_size:
            self.pre_trace = torch.zeros(batch_size, self.in_features, device=pre_spike.device)
            self.post_trace = torch.zeros(batch_size, self.out_features, device=post_spike.device)
        
        # ã‚·ãƒŠãƒ—ã‚¹å‡ºåŠ›è¨ˆç®—
        output = F.linear(pre_spike, self.weight)
        
        if learning:
            self._update_stdp(pre_spike, post_spike)
        
        # ãƒˆãƒ¬ãƒ¼ã‚¹æ›´æ–°ï¼ˆæŒ‡æ•°æ¸›è¡° + æ–°ã—ã„ã‚¹ãƒ‘ã‚¤ã‚¯ï¼‰
        self.pre_trace = self.pre_trace * self.pre_decay + pre_spike
        self.post_trace = self.post_trace * self.post_decay + post_spike
        
        return output
    
    def _update_stdp(self, pre_spike: torch.Tensor, post_spike: torch.Tensor):
        """STDPé‡ã¿æ›´æ–°"""
        # LTP (Long-Term Potentiation): ãƒã‚¹ãƒˆã‚¹ãƒ‘ã‚¤ã‚¯ãŒãƒ—ãƒ¬ãƒˆãƒ¬ãƒ¼ã‚¹ã¨ç›¸é–¢
        # pre_trace ã¯éå»ã®ãƒ—ãƒ¬ã‚¹ãƒ‘ã‚¤ã‚¯ã®å±¥æ­´ã€post_spike ã¯ç¾åœ¨ã®ãƒã‚¹ãƒˆã‚¹ãƒ‘ã‚¤ã‚¯
        ltp_update = torch.outer(post_spike.mean(0), self.pre_trace.mean(0)) * self.A_pos
        
        # LTD (Long-Term Depression): ãƒ—ãƒ¬ã‚¹ãƒ‘ã‚¤ã‚¯ãŒãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ã‚¹ã¨ç›¸é–¢  
        # pre_spike ã¯ç¾åœ¨ã®ãƒ—ãƒ¬ã‚¹ãƒ‘ã‚¤ã‚¯ã€post_trace ã¯éå»ã®ãƒã‚¹ãƒˆã‚¹ãƒ‘ã‚¤ã‚¯ã®å±¥æ­´
        ltd_update = torch.outer(self.post_trace.mean(0), pre_spike.mean(0)) * self.A_neg
        
        # é‡ã¿æ›´æ–°
        delta_w = ltp_update - ltd_update
        
        # ãƒ›ãƒ¡ã‚ªã‚¹ã‚¿ã‚·ã‚¹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        if self.homeostatic_scaling:
            delta_w = self._apply_homeostatic_scaling(delta_w, pre_spike, post_spike)
        
        # é‡ã¿æ›´æ–°ã¨åˆ¶ç´„
        with torch.no_grad():
            self.weight.data += delta_w
            self.weight.data.clamp_(self.w_min, self.w_max)
    
    def _apply_homeostatic_scaling(self, delta_w: torch.Tensor, 
                                 pre_spike: torch.Tensor, post_spike: torch.Tensor) -> torch.Tensor:
        """ãƒ›ãƒ¡ã‚ªã‚¹ã‚¿ã‚·ã‚¹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼šç™ºç«ç‡ã‚’ç›®æ¨™å€¤ã«ç¶­æŒ"""
        # æ´»å‹•ãƒ¬ãƒ¼ãƒˆæ›´æ–°ï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰
        current_pre_rate = pre_spike.mean(0)
        current_post_rate = post_spike.mean(0)
        
        self.pre_rate = (1 - self.homeostatic_alpha) * self.pre_rate + self.homeostatic_alpha * current_pre_rate
        self.post_rate = (1 - self.homeostatic_alpha) * self.post_rate + self.homeostatic_alpha * current_post_rate
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼è¨ˆç®—
        pre_scaling = self.target_rate / (self.pre_rate + 1e-6)
        post_scaling = self.target_rate / (self.post_rate + 1e-6)
        
        # é‡ã¿æ›´æ–°ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨
        scaling_matrix = torch.outer(post_scaling, pre_scaling)
        return delta_w * scaling_matrix.sqrt()

# ----------------------------------------
# 2. çŸ­æœŸã‚·ãƒŠãƒ—ã‚¹å¯å¡‘æ€§ (STP)
# ----------------------------------------

class STPSynapse(nn.Module):
    """
    çŸ­æœŸã‚·ãƒŠãƒ—ã‚¹å¯å¡‘æ€§ - ä¿ƒé€²ã¨æŠ‘åœ§ã®å‹•çš„åˆ¶å¾¡
    æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã®ç•°ãªã‚‹é©å¿œã‚’å®Ÿç¾
    """
    def __init__(self, 
                 in_features: int, 
                 out_features: int,
                 tau_fac: float = 100.0,     # ä¿ƒé€²ã®æ™‚å®šæ•°
                 tau_dep: float = 200.0,     # æŠ‘åœ§ã®æ™‚å®šæ•°
                 U: float = 0.5,             # ä½¿ç”¨ç‡
                 use_facilitation: bool = True,
                 use_depression: bool = True):
        super().__init__()
        
        self.in_features = in_features
        self.out_features = out_features
        self.tau_fac = tau_fac
        self.tau_dep = tau_dep
        self.U = U
        self.use_facilitation = use_facilitation
        self.use_depression = use_depression
        
        # åŸºæœ¬é‡ã¿
        self.weight = nn.Parameter(torch.rand(out_features, in_features) * 0.5 + 0.25)
        
        # STPçŠ¶æ…‹å¤‰æ•°
        if use_facilitation:
            self.register_buffer('u', torch.ones(1, in_features) * U)  # ä¿ƒé€²å¤‰æ•°
        if use_depression:
            self.register_buffer('x', torch.ones(1, in_features))      # ãƒªã‚½ãƒ¼ã‚¹å¤‰æ•°
        
        # æ¸›è¡°ä¿‚æ•°
        if use_facilitation:
            self.fac_decay = math.exp(-1.0 / tau_fac)
        if use_depression:
            self.dep_decay = math.exp(-1.0 / tau_dep)
    
    def forward(self, pre_spike: torch.Tensor) -> torch.Tensor:
        batch_size = pre_spike.shape[0]
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
        if self.use_facilitation and self.u.shape[0] != batch_size:
            self.u = torch.ones(batch_size, self.in_features, device=pre_spike.device) * self.U
        if self.use_depression and self.x.shape[0] != batch_size:
            self.x = torch.ones(batch_size, self.in_features, device=pre_spike.device)
        
        # ç¾åœ¨ã®æœ‰åŠ¹é‡ã¿è¨ˆç®—
        effective_weight = self.weight.clone()
        
        if self.use_facilitation and self.use_depression:
            # ä¿ƒé€²ã¨æŠ‘åœ§ã®ä¸¡æ–¹
            release_prob = self.u * self.x
            effective_weight = effective_weight * release_prob.unsqueeze(0)
            
            # çŠ¶æ…‹æ›´æ–°
            self.u = self.u * self.fac_decay + self.U * (1 - self.u * self.fac_decay) * pre_spike
            self.x = self.x * self.dep_decay + (1 - self.x) * self.dep_decay * (1 - pre_spike * self.u)
            
        elif self.use_facilitation:
            # ä¿ƒé€²ã®ã¿
            effective_weight = effective_weight * self.u.unsqueeze(0)
            self.u = self.u * self.fac_decay + self.U * (1 - self.u * self.fac_decay) * pre_spike
            
        elif self.use_depression:
            # æŠ‘åœ§ã®ã¿
            effective_weight = effective_weight * self.x.unsqueeze(0)
            self.x = self.x * self.dep_decay + (1 - self.x) * (1 - pre_spike)
        
        return F.linear(pre_spike, effective_weight)

# ----------------------------------------
# 3. ãƒ¡ã‚¿å¯å¡‘æ€§ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³
# ----------------------------------------

class MetaplasticLIFNeuron(nn.Module):
    """
    ãƒ¡ã‚¿å¯å¡‘æ€§ã‚’æŒã¤LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³
    å­¦ç¿’å±¥æ­´ã«åŸºã¥ã„ã¦å¯å¡‘æ€§ã‚’å‹•çš„ã«èª¿æ•´
    """
    def __init__(self, 
                 features: int,
                 tau: float = 2.0,
                 threshold: float = 1.0,
                 metaplastic_tau: float = 1000.0,  # ãƒ¡ã‚¿å¯å¡‘æ€§ã®æ™‚å®šæ•°
                 metaplastic_strength: float = 0.1):
        super().__init__()
        
        self.features = features
        self.tau = tau
        self.base_threshold = threshold
        self.metaplastic_tau = metaplastic_tau
        self.metaplastic_strength = metaplastic_strength
        
        # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®çŠ¶æ…‹
        self.register_buffer('v_mem', torch.zeros(1, features))
        self.register_buffer('activity_history', torch.zeros(1, features))  # ãƒ¡ã‚¿å¯å¡‘æ€§ç”¨
        
        # é©å¿œçš„é–¾å€¤ï¼ˆå­¦ç¿’ã«åŸºã¥ã„ã¦å¤‰åŒ–ï¼‰
        self.register_buffer('adaptive_threshold', torch.ones(features) * threshold)
        
        # ä»£ç†å‹¾é…
        self.surrogate_function = surrogate.ATan(alpha=2.0)
        
        # æ¸›è¡°ä¿‚æ•°
        self.mem_decay = math.exp(-1.0 / tau)
        self.meta_decay = math.exp(-1.0 / metaplastic_tau)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.shape[0]
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
        if self.v_mem.shape[0] != batch_size:
            self.v_mem = torch.zeros(batch_size, self.features, device=x.device)
            self.activity_history = torch.zeros(batch_size, self.features, device=x.device)
        
        # è†œé›»ä½æ›´æ–°
        self.v_mem = self.v_mem * self.mem_decay + x
        
        # ãƒ¡ã‚¿å¯å¡‘æ€§ã«åŸºã¥ãé©å¿œçš„é–¾å€¤
        current_threshold = self.adaptive_threshold * (1.0 + self.metaplastic_strength * self.activity_history.mean(0))
        
        # ã‚¹ãƒ‘ã‚¤ã‚¯ç™ºç”Ÿ
        spike = self.surrogate_function(self.v_mem - current_threshold)
        
        # ç™ºç«å¾Œãƒªã‚»ãƒƒãƒˆ
        self.v_mem = self.v_mem * (1.0 - spike.detach())
        
        # æ´»å‹•å±¥æ­´æ›´æ–°ï¼ˆãƒ¡ã‚¿å¯å¡‘æ€§ç”¨ï¼‰
        self.activity_history = self.activity_history * self.meta_decay + spike.detach() * (1 - self.meta_decay)
        
        return spike

# ----------------------------------------
# 4. ç”Ÿç‰©å­¦çš„å¯å¡‘æ€§çµ±åˆSNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
# ----------------------------------------

class BioplasticSNN(nn.Module):
    """
    ç”Ÿç‰©å­¦çš„ã‚·ãƒŠãƒ—ã‚¹å¯å¡‘æ€§ã‚’çµ±åˆã—ãŸSNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
    """
    def __init__(self, 
                 vocab_size: int, 
                 d_model: int = 256,
                 num_layers: int = 3,
                 time_steps: int = 20,
                 use_stdp: bool = True,
                 use_stp: bool = True,
                 use_metaplasticity: bool = True):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.time_steps = time_steps
        self.use_stdp = use_stdp
        self.use_stp = use_stp
        self.use_metaplasticity = use_metaplasticity
        
        # åŸ‹ã‚è¾¼ã¿å±¤
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        # ã‚¹ãƒ‘ã‚¤ã‚¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆTTFSãƒ™ãƒ¼ã‚¹ï¼‰
        from snn_advanced_optimization import TTFSEncoder
        self.spike_encoder = TTFSEncoder(d_model, time_steps)
        
        # å¯å¡‘æ€§ã‚·ãƒŠãƒ—ã‚¹å±¤
        self.plastic_layers = nn.ModuleList()
        for i in range(num_layers):
            layer_dict = nn.ModuleDict()
            
            if use_stdp:
                layer_dict['stdp_synapse'] = STDPSynapse(d_model, d_model)
            if use_stp:
                layer_dict['stp_synapse'] = STPSynapse(d_model, d_model)
            if use_metaplasticity:
                layer_dict['metaplastic_neuron'] = MetaplasticLIFNeuron(d_model)
            else:
                # æ¨™æº–LIF
                layer_dict['lif_neuron'] = neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan())
            
            self.plastic_layers.append(layer_dict)
        
        # å‡ºåŠ›å±¤
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # ãƒªã‚¶ãƒãƒ¼çŠ¶æ…‹ï¼ˆè‡ªå·±çµ„ç¹”åŒ–ç”¨ï¼‰
        self.register_buffer('reservoir_state', torch.zeros(1, d_model))
        
    def forward(self, input_ids: torch.Tensor, learning_mode: bool = True) -> torch.Tensor:
        batch_size, seq_len = input_ids.shape
        
        # åŸ‹ã‚è¾¼ã¿
        token_emb = self.token_embedding(input_ids)
        
        # ã‚¹ãƒ‘ã‚¤ã‚¯ç¬¦å·åŒ–
        spike_sequence = self.spike_encoder(token_emb)
        
        # å¯å¡‘æ€§å±¤ã§ã®å‡¦ç†
        hidden_states = spike_sequence
        
        for t in range(self.time_steps):
            current_spikes = hidden_states[:, t, :, :]  # (batch, seq_len, d_model)
            
            layer_output = current_spikes
            for layer in self.plastic_layers:
                if 'stdp_synapse' in layer:
                    # STDPå­¦ç¿’ï¼ˆå‰ã®å±¤ã®å‡ºåŠ›ã‚’ä½¿ç”¨ï¼‰
                    pre_spikes = layer_output
                    post_spikes = layer['metaplastic_neuron'](layer_output) if 'metaplastic_neuron' in layer else layer_output
                    layer_output = layer['stdp_synapse'](pre_spikes.view(-1, self.d_model), 
                                                       post_spikes.view(-1, self.d_model), 
                                                       learning=learning_mode).view(batch_size, seq_len, self.d_model)
                
                if 'stp_synapse' in layer:
                    # çŸ­æœŸå¯å¡‘æ€§
                    layer_output = layer['stp_synapse'](layer_output.view(-1, self.d_model)).view(batch_size, seq_len, self.d_model)
                
                if 'metaplastic_neuron' in layer:
                    # ãƒ¡ã‚¿å¯å¡‘æ€§ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³
                    layer_output = layer['metaplastic_neuron'](layer_output.view(-1, self.d_model)).view(batch_size, seq_len, self.d_model)
            
            hidden_states[:, t, :, :] = layer_output
        
        # æ™‚é–“çµ±åˆ
        time_integrated = hidden_states.mean(dim=1)  # (batch, seq_len, d_model)
        
        # å‡ºåŠ›æŠ•å½±
        logits = self.output_projection(time_integrated)
        
        return logits

# ----------------------------------------
# 5. é©å¿œçš„å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
# ----------------------------------------

class AdaptivePlasticityTrainer:
    """
    ç”Ÿç‰©å­¦çš„å¯å¡‘æ€§ã‚’æŒã¤SNNã®é©å¿œçš„å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
    """
    def __init__(self, 
                 model: BioplasticSNN,
                 base_lr: float = 1e-4,
                 plasticity_lr: float = 1e-3,
                 device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        
        self.model = model.to(device)
        self.device = device
        
        # ç•°ãªã‚‹å­¦ç¿’ç‡ã§ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’åˆ†é›¢
        # åŸ‹ã‚è¾¼ã¿ã¨å‡ºåŠ›å±¤ï¼šä½å­¦ç¿’ç‡
        backbone_params = list(self.model.token_embedding.parameters()) + list(self.model.output_projection.parameters())
        self.backbone_optimizer = torch.optim.AdamW(backbone_params, lr=base_lr)
        
        # å¯å¡‘æ€§å±¤ï¼šé«˜å­¦ç¿’ç‡ï¼ˆç”Ÿç‰©å­¦çš„å­¦ç¿’ã®é«˜é€Ÿæ€§ã‚’æ¨¡å€£ï¼‰
        plasticity_params = []
        for layer in self.model.plastic_layers:
            plasticity_params.extend(layer.parameters())
        self.plasticity_optimizer = torch.optim.AdamW(plasticity_params, lr=plasticity_lr)
        
        # æå¤±é–¢æ•°ï¼ˆå¯å¡‘æ€§ã‚’è€ƒæ…®ï¼‰
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # é©å¿œçš„å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        self.backbone_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.backbone_optimizer, T_0=50, T_mult=2)
        self.plasticity_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.plasticity_optimizer, T_0=20, T_mult=1.5)
        
    def train_step(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> Dict[str, float]:
        """ç”Ÿç‰©å­¦çš„å¯å¡‘æ€§ã‚’è€ƒæ…®ã—ãŸå­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—"""
        input_ids, target_ids = [t.to(self.device) for t in batch]
        
        self.model.train()
        
        # Forward passï¼ˆå¯å¡‘æ€§å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼‰
        logits = self.model(input_ids, learning_mode=True)
        
        # æå¤±è¨ˆç®—
        loss = self.criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))
        
        # Backward pass
        self.backbone_optimizer.zero_grad()
        self.plasticity_optimizer.zero_grad()
        
        loss.backward()
        
        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¯å¡‘æ€§ã®å®‰å®šåŒ–ï¼‰
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼æ›´æ–°
        self.backbone_optimizer.step()
        self.plasticity_optimizer.step()
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ›´æ–°
        self.backbone_scheduler.step()
        self.plasticity_scheduler.step()
        
        return {'loss': loss.item()}

# ----------------------------------------
# 6. ä½¿ç”¨ä¾‹ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
# ----------------------------------------

def test_bioplastic_snn():
    """ç”Ÿç‰©å­¦çš„å¯å¡‘æ€§SNNã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§  ç”Ÿç‰©å­¦çš„å¯å¡‘æ€§SNNã®ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹...")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    vocab_size = 1000
    d_model = 128
    batch_size = 4
    seq_len = 16
    time_steps = 12
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = BioplasticSNN(
        vocab_size=vocab_size,
        d_model=d_model,
        num_layers=3,
        time_steps=time_steps,
        use_stdp=True,
        use_stp=True,
        use_metaplasticity=True
    )
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_input = torch.randint(0, vocab_size, (batch_size, seq_len))
    test_targets = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # å­¦ç¿’å‰ã®æ¨è«–
    model.eval()
    with torch.no_grad():
        logits_before = model(test_input, learning_mode=False)
        initial_acc = (logits_before.argmax(-1) == test_targets).float().mean()
    
    print(f"åˆæœŸç²¾åº¦: {initial_acc:.3f}")
    
    # é©å¿œçš„å­¦ç¿’
    trainer = AdaptivePlasticityTrainer(model)
    
    print("ğŸ”„ ç”Ÿç‰©å­¦çš„å¯å¡‘æ€§ã«ã‚ˆã‚‹é©å¿œå­¦ç¿’...")
    for step in range(50):
        metrics = trainer.train_step((test_input, test_targets))
        if (step + 1) % 10 == 0:
            print(f"Step {step+1}: Loss = {metrics['loss']:.4f}")
    
    # å­¦ç¿’å¾Œã®æ¨è«–
    model.eval()
    with torch.no_grad():
        logits_after = model(test_input, learning_mode=False)
        final_acc = (logits_after.argmax(-1) == test_targets).float().mean()
    
    print(f"æœ€çµ‚ç²¾åº¦: {final_acc:.3f}")
    print(f"ç²¾åº¦å‘ä¸Š: {final_acc - initial_acc:.3f}")
    
    # å¯å¡‘æ€§ã®åˆ†æ
    print("\nğŸ“Š ã‚·ãƒŠãƒ—ã‚¹å¯å¡‘æ€§åˆ†æ:")
    for i, layer in enumerate(model.plastic_layers):
        if 'stdp_synapse' in layer:
            stdp_weights = layer['stdp_synapse'].weight.data
            print(f"Layer {i} STDPé‡ã¿ç¯„å›²: [{stdp_weights.min():.3f}, {stdp_weights.max():.3f}]")
            print(f"Layer {i} STDPé‡ã¿å¹³å‡: {stdp_weights.mean():.3f}")
    
    print("âœ… ç”Ÿç‰©å­¦çš„å¯å¡‘æ€§SNNãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    test_bioplastic_snn()