# matsushibadenki/snn/benchmark/sst2_benchmark.pyã®ä¿®æ­£
#
# GLUEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (SST-2ã‚¿ã‚¹ã‚¯) ã‚’ç”¨ã„ãŸSNNãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
#
# ç›®çš„:
# - ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— ãƒ•ã‚§ãƒ¼ã‚º1ã€Œ1.1. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç’°å¢ƒã®æ§‹ç¯‰ã€ã€Œ1.2. ANNãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒã€ã«å¯¾å¿œã€‚
# - æ¨™æº–çš„ãªNLPã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹SNNãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ã€ANNãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦å®¢è¦³çš„ã‹ã¤å®šé‡çš„ã«è©•ä¾¡ã™ã‚‹ã€‚
#
# æ©Ÿèƒ½:
# 1. Hugging Face `datasets`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰SST-2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‚
# 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’SNN/ANNãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’å¯èƒ½ãªå½¢å¼ã«å‰å‡¦ç†ãƒ»å¤‰æ›ã€‚
# 3. SNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡ã‚’å®Ÿè¡Œã€‚
# 4. ANNãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡ã‚’å®Ÿè¡Œã€‚
# 5. ä¸¡ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æŒ‡æ¨™ï¼ˆæ­£è§£ç‡ã€æ¨è«–æ™‚é–“ï¼‰ã‚’ä¸¦ã¹ã¦è¡¨ç¤ºã—ã€æ¯”è¼ƒã‚’å®¹æ˜“ã«ã™ã‚‹ã€‚

import os
import json
import time
import pandas as pd
from datasets import load_dataset
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import sys
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’sys.pathã«è¿½åŠ ã—ã¦ã€mainã‚„snn_coreã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ã«ã™ã‚‹
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from main import Vocabulary, collate_fn
from snn_core import BreakthroughSNN, BreakthroughTrainer, CombinedLoss
from benchmark.ann_baseline import ANNBaselineModel

# ----------------------------------------
# 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
# ----------------------------------------
def prepare_sst2_data(output_dir: str = "data"):
    """
    SST-2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€.jsonlå½¢å¼ã«å¤‰æ›ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    print("Downloading SST-2 dataset...")
    dataset = load_dataset("glue", "sst2")

    data_paths = {}
    for split in ["train", "validation"]: # testã‚¹ãƒ—ãƒªãƒƒãƒˆã¯ãƒ©ãƒ™ãƒ«ãŒãªã„ãŸã‚é™¤å¤–
        jsonl_path = os.path.join(output_dir, f"sst2_{split}.jsonl")
        data_paths[split] = jsonl_path
        
        if os.path.exists(jsonl_path):
            print(f"'{split}' split already exists at {jsonl_path}. Skipping preparation.")
            continue
            
        print(f"Processing '{split}' split -> {jsonl_path}")
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for example in tqdm(dataset[split]):
                record = {
                    "sentence": example['sentence'],
                    "label": example['label'] 
                }
                f.write(json.dumps(record) + "\n")
    
    print("âœ… SST-2 data preparation complete.")
    return data_paths

# ----------------------------------------
# 2. SNNãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
# ----------------------------------------
def run_snn_benchmark(data_paths: dict, model_path: str, vocab: Vocabulary):
    """æº–å‚™ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§SNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡ã‚’è¡Œã†ã€‚"""
    print("\n" + "="*20 + " ğŸš€ Starting SNN Benchmark " + "="*20)

    # --- 1. Training ---
    print("\nğŸ”¥ Step 1: Training the SNN model on SST-2 train set...")
    train_args = type('Args', (), {
        'data_path': data_paths['train'],
        'data_format': 'instruction', # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’instructionã«å½è£…ã—ã¦æµç”¨
        'epochs': 3, 
        'batch_size': 16,
        'learning_rate': 1e-4,
        'log_interval': 1,
        'model_path': model_path,
        'd_model': 64, 'd_state': 32, 'num_layers': 2, 'time_steps': 16
    })()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’Instructionå½¢å¼ã«å¤‰æ›ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼
    def convert_to_instruction_format(original_path, new_path):
        with open(original_path, 'r') as fin, open(new_path, 'w') as fout:
            for line in fin:
                item = json.loads(line)
                instruction = "Classify the sentiment of the following sentence."
                output = "positive" if item['label'] == 1 else "negative"
                fout.write(json.dumps({"instruction": instruction, "input": item['sentence'], "output": output}) + "\n")
    
    train_inst_path = data_paths['train'].replace('.jsonl', '_inst.jsonl')
    convert_to_instruction_format(data_paths['train'], train_inst_path)
    train_args.data_path = train_inst_path
    
    run_training(train_args, vocab)
    print("âœ… SNN Model training complete.")

    # --- 2. Evaluation ---
    print("\nğŸ“Š Step 2: Evaluating the SNN model on SST-2 validation set...")
    engine = SNNInferenceEngine(model_path=model_path)
    
    true_labels, pred_labels, latencies = [], [], []

    with open(data_paths['validation'], 'r', encoding='utf-8') as f:
        validation_data = [json.loads(line) for line in f]

    for item in tqdm(validation_data, desc="SNN Evaluating"):
        prompt = f"Classify the sentiment of the following sentence.\n{item['sentence']}"
        
        start_time = time.time()
        generated_text = engine.generate(prompt, max_len=3)
        latencies.append((time.time() - start_time) * 1000) # ms

        pred_labels.append(1 if "positive" in generated_text else 0)
        true_labels.append(item['label'])
        
    # --- 3. Calculate Metrics ---
    accuracy = accuracy_score(true_labels, pred_labels)
    avg_latency = sum(latencies) / len(latencies)
    
    print(f"  SNN Validation Accuracy: {accuracy:.4f}")
    print(f"  SNN Average Inference Time: {avg_latency:.2f} ms")
    return {"model": "BreakthroughSNN", "accuracy": accuracy, "avg_latency_ms": avg_latency}

# SNNç”¨ã®åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
class SNNClassificationDataset(Dataset):
    def __init__(self, file_path, vocab):
        self.vocab = vocab
        self.data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line)
                # SNNãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã¯ (input, target) ã®ãƒšã‚¢
                encoded = self.vocab.encode(item['sentence'])
                # ãƒ©ãƒ™ãƒ«æƒ…å ±ã‚’æå¤±è¨ˆç®—ã®ãŸã‚ã«åˆ¥é€”ä¿æŒ
                self.data.append({'encoded': encoded, 'label': item['label']})

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        encoded = item['encoded']
        # SNNã¯æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã§äº‹å‰å­¦ç¿’ã™ã‚‹ãŸã‚ã€å…¥åŠ›ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ç”Ÿæˆ
        # åˆ†é¡ã‚¿ã‚¹ã‚¯ã§ã¯æœ€å¾Œã®å‡ºåŠ›ã®ã¿ã‚’ä½¿ã†ãŒã€å­¦ç¿’å½¢å¼ã¯åˆã‚ã›ã‚‹
        return torch.tensor(encoded[:-1]), torch.tensor(encoded[1:], dtype=torch.long)

# SNNã®åˆ†é¡ãƒ˜ãƒƒãƒ‰
class SNNClassifier(nn.Module):
    def __init__(self, snn_backbone, d_model, num_classes):
        super().__init__()
        self.snn_backbone = snn_backbone
        self.classifier = nn.Linear(d_model, num_classes)
    
    def forward(self, input_ids):
        # SNNãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã‹ã‚‰ç‰¹å¾´é‡ã‚’å–å¾—
        logits, spikes = self.snn_backbone(input_ids, return_spikes=True)
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã®æ™‚é–“ç©åˆ†ã•ã‚ŒãŸç‰¹å¾´é‡ã‚’ä½¿ç”¨
        time_integrated_features = spikes.mean(dim=1)
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ–¹å‘ã§å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°
        pooled_features = time_integrated_features.mean(dim=1)
        # åˆ†é¡
        class_logits = self.classifier(pooled_features)
        return class_logits, spikes


def run_snn_benchmark(data_paths: dict, model_path: str, vocab: Vocabulary):
    """æº–å‚™ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§SNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡ã‚’è¡Œã†ã€‚"""
    print("\n" + "="*20 + " ğŸš€ Starting SNN Benchmark " + "="*20)
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # --- 1. DataLoaders ---
    # ã“ã“ã§ã¯ã€SNNã‚‚ANNã¨åŒæ§˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå½¢å¼ã§æ‰±ã†
    train_dataset = SNNClassificationDataset(data_paths['train'], vocab)
    val_dataset = SNNClassificationDataset(data_paths['validation'], vocab)

    # (DataLoaderã®å®šç¾©ã¯ANNã¨å…±é€šåŒ–å¯èƒ½)
    # ...

    # --- 2. Model, Optimizer, Loss ---
    snn_backbone = BreakthroughSNN(vocab_size=vocab.vocab_size, d_model=64, d_state=32, num_layers=2, time_steps=16)
    model = SNNClassifier(snn_backbone, d_model=64, num_classes=2).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    # åˆ†é¡ãªã®ã§CrossEntropyLossã‚’ç›´æ¥ä½¿ã†
    criterion = nn.CrossEntropyLoss()
    
    # --- 3. Training Loop (å°‚ç”¨ã®ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…) ---
    print("\nğŸ”¥ Step 1: Training the SNN model for classification...")
    for epoch in range(3):
        model.train()
        # (ã“ã“ã«å°‚ç”¨ã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…)
    
    # --- 4. Evaluation (å°‚ç”¨ã®è©•ä¾¡ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…) ---
    print("\nğŸ“Š Step 2: Evaluating the SNN model...")
    # (ã“ã“ã«å°‚ç”¨ã®è©•ä¾¡ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…)

    # (çµæœã®è¨ˆç®—ã¨è¿”å´)
    # ...
    return {"model": "BreakthroughSNN", "accuracy": 0.0, "avg_latency_ms": 0.0} # ãƒ€ãƒŸãƒ¼

# ----------------------------------------
# 3. ANNãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
# ----------------------------------------
class SST2Dataset(Dataset):
    def __init__(self, file_path, vocab):
        self.vocab = vocab
        with open(file_path, 'r', encoding='utf-8') as f:
            self.data = [json.loads(line) for line in f]
    def __len__(self): return len(self.data)
    def __getitem__(self, idx):
        text = self.data[idx]['sentence']
        encoded = self.vocab.encode(text, add_start_end=False)
        return torch.tensor(encoded, dtype=torch.long), self.data[idx]['label']

def run_ann_training_and_eval(data_paths: dict, vocab: Vocabulary, model_params: dict):
    """ANNãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡ã‚’è¡Œã†ã€‚"""
    print("\n" + "="*20 + " ğŸ“Š Starting ANN Benchmark " + "="*20)
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # --- 1. DataLoaders ---
    train_dataset = SST2Dataset(data_paths['train'], vocab)
    val_dataset = SST2Dataset(data_paths['validation'], vocab)

    def ann_collate_fn(batch):
        inputs, targets = zip(*batch)
        padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=vocab.pad_id)
        return padded_inputs, torch.tensor(targets, dtype=torch.long)

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=ann_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=ann_collate_fn)

    # --- 2. Model, Optimizer, Loss ---
    model = ANNBaselineModel(vocab_size=vocab.vocab_size, **model_params).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()
    
    print(f"ANN Model Parameters: {sum(p.numel() for p in model.parameters())}")

    # --- 3. Training Loop ---
    print("\nğŸ”¥ Step 1: Training the ANN model...")
    for epoch in range(3): # SNNã¨ã‚¨ãƒãƒƒã‚¯æ•°ã‚’åˆã‚ã›ã‚‹
        model.train()
        for inputs, targets in tqdm(train_loader, desc=f"ANN Epoch {epoch+1}"):
            inputs, targets = inputs.to(device), targets.to(device)
            padding_mask = (inputs == vocab.pad_id)
            
            optimizer.zero_grad()
            outputs = model(inputs, padding_mask)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
    print("âœ… ANN Model training complete.")
            
    # --- 4. Evaluation ---
    print("\nğŸ“Š Step 2: Evaluating the ANN model...")
    model.eval()
    true_labels, pred_labels, latencies = [], [], []
    with torch.no_grad():
        for inputs, targets in tqdm(val_loader, desc="ANN Evaluating"):
            inputs, targets = inputs.to(device), targets.to(device)
            padding_mask = (inputs == vocab.pad_id)
            
            start_time = time.time()
            outputs = model(inputs, padding_mask)
            latencies.append((time.time() - start_time) * 1000)
            
            preds = torch.argmax(outputs, dim=1)
            pred_labels.extend(preds.cpu().numpy())
            true_labels.extend(targets.cpu().numpy())
    
    # --- 5. Calculate Metrics ---
    accuracy = accuracy_score(true_labels, pred_labels)
    # 1ãƒãƒƒãƒã‚ãŸã‚Šã®å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
    avg_latency = sum(latencies) / len(latencies)

    print(f"  ANN Validation Accuracy: {accuracy:.4f}")
    print(f"  ANN Average Inference Time (per batch): {avg_latency:.2f} ms")
    return {"model": "ANN Baseline", "accuracy": accuracy, "avg_latency_ms": avg_latency}

# ----------------------------------------
# 4. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯
# ----------------------------------------
if __name__ == "__main__":
    # --- æº–å‚™ ---
    pd.set_option('display.precision', 4)
    data_paths = prepare_sst2_data()
    snn_model_path = "breakthrough_snn_sst2.pth"
    
    # å…±é€šã®èªå½™ã‚’æ§‹ç¯‰
    vocab = Vocabulary()
    print("\nğŸ“– Building shared vocabulary from training data...")
    with open(data_paths['train'], 'r', encoding='utf-8') as f:
        all_texts = (json.loads(line)['sentence'] for line in f)
        vocab.build_vocab(all_texts)
    print(f"âœ… Vocabulary built. Size: {vocab.vocab_size}")

    # --- SNNãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ ---
    snn_results = run_snn_benchmark(data_paths, snn_model_path, vocab)

    # --- ANNãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ ---
    ann_model_params = {'d_model': 64, 'nhead': 2, 'd_hid': 128, 'nlayers': 2}
    ann_results = run_ann_training_and_eval(data_paths, vocab, ann_model_params)
    
    # --- çµæœã®æ¯”è¼ƒ ---
    print("\n\n" + "="*25 + " ğŸ† Final Benchmark Results " + "="*25)
    results_df = pd.DataFrame([snn_results, ann_results])
    print(results_df.to_string(index=False))
    print("="*75)
    
    # æ³¨: SNNã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¯1æ–‡ã”ã¨ã€ANNã¯1ãƒãƒƒãƒã”ã¨ã®ãŸã‚ç›´æ¥æ¯”è¼ƒã¯ã§ããªã„ç‚¹ã«æ³¨æ„ã€‚
    # ANNã®æ–¹ãŒãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¼”ç®—ã«ã‚ˆã‚Šãƒãƒƒãƒå‡¦ç†ã§é«˜é€Ÿã«ãªã‚‹å‚¾å‘ãŒã‚ã‚‹ã€‚
