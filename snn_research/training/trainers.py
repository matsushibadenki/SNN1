# matsushibadenki/snn/snn_research/training/trainers.py
# SNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡ãƒ«ãƒ¼ãƒ—ã‚’ç®¡ç†ã™ã‚‹Trainerã‚¯ãƒ©ã‚¹ (ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»è©•ä¾¡æ©Ÿèƒ½å®Œå‚™)
# 
# æ©Ÿèƒ½:
# - TensorBoardã¨é€£æºã—ã€å­¦ç¿’ãƒ»æ¤œè¨¼ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å¯è¦–åŒ–ã€‚
# - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ `evaluate` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã€‚
# - æ¤œè¨¼çµæœã«åŸºã¥ãã€æœ€ã‚‚æ€§èƒ½ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã‚’ `best_model.pth` ã¨ã—ã¦ä¿å­˜ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
# - GradScalerã®éæ¨å¥¨è­¦å‘Šã‚’ä¿®æ­£ã€‚
# - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ã—ã€ãƒãƒƒãƒ•ã‚¡ã‚’é™¤å¤–ã—ã¦å†é–‹æ™‚ã®ã‚µã‚¤ã‚ºãƒŸã‚¹ãƒãƒƒãƒã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã€‚

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import os
import collections
from tqdm import tqdm  # type: ignore
from typing import Tuple, Dict, Any, Optional
import shutil

from snn_research.training.losses import CombinedLoss, DistillationLoss
from torch.utils.tensorboard import SummaryWriter

class BreakthroughTrainer:
    """ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨è©•ä¾¡æ©Ÿèƒ½ã‚’å®Œå‚™ã—ãŸã€SNNã®çµ±åˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã€‚"""
    def __init__(self, model: nn.Module, optimizer: torch.optim.Optimizer, criterion: nn.Module,
                 scheduler: Optional[torch.optim.lr_scheduler.LRScheduler], device: str,
                 grad_clip_norm: float, rank: int, use_amp: bool, log_dir: str):
        self.model = model
        self.device = device
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.criterion = criterion
        self.grad_clip_norm = grad_clip_norm
        self.rank = rank
        self.use_amp = use_amp and torch.cuda.is_available()
        # éæ¨å¥¨è­¦å‘Šã‚’ä¿®æ­£
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp) if self.device == 'cuda' else torch.amp.GradScaler('cpu', enabled=self.use_amp)
        self.best_metric = float('inf') # æå¤±ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã€ç„¡é™å¤§ã§åˆæœŸåŒ–
        
        # rank 0 ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿ãŒTensorBoardã®æ›¸ãè¾¼ã¿ã¨ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ã‚’è¡Œã†
        if self.rank in [-1, 0]:
            self.writer = SummaryWriter(log_dir)
            print(f"âœ… TensorBoard logging enabled. Log directory: {log_dir}")

    def _run_step(self, batch: Tuple[torch.Tensor, ...], is_train: bool) -> Dict[str, Any]:
        if is_train:
            self.model.train()
        else:
            self.model.eval()

        input_ids, target_ids = [t.to(self.device) for t in batch[:2]]
        
        with torch.set_grad_enabled(is_train):
            with torch.cuda.amp.autocast(enabled=self.use_amp):
                logits, spike_data = self.model(input_ids, return_spikes=True)
                loss_dict = self.criterion(logits, target_ids, spike_data)
        
        if is_train:
            self.optimizer.zero_grad()
            self.scaler.scale(loss_dict['total']).backward()
            if self.grad_clip_norm > 0:
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_norm)
            self.scaler.step(self.optimizer)
            self.scaler.update()
        
        with torch.no_grad():
            preds = torch.argmax(logits, dim=-1)
            if isinstance(self.criterion, CombinedLoss):
                ignore_idx = self.criterion.ce_loss_fn.ignore_index
                mask = target_ids != ignore_idx
                accuracy = (preds[mask] == target_ids[mask]).sum().float() / mask.sum() if mask.sum() > 0 else torch.tensor(0.0)
                loss_dict['accuracy'] = accuracy

        return {k: v.item() if torch.is_tensor(v) else v for k, v in loss_dict.items()}

    def train_epoch(self, dataloader: DataLoader, epoch: int) -> Dict[str, float]:
        total_metrics: Dict[str, float] = collections.defaultdict(float)
        num_batches = len(dataloader)
        progress_bar = tqdm(dataloader, desc=f"Training Epoch {epoch}", disable=(self.rank not in [-1, 0]))
        
        for batch in progress_bar:
            metrics = self._run_step(batch, is_train=True)
            for key, value in metrics.items(): total_metrics[key] += value

        if self.scheduler: self.scheduler.step()
        
        avg_metrics = {key: value / num_batches for key, value in total_metrics.items()}
        
        if self.rank in [-1, 0]:
            for key, value in avg_metrics.items():
                self.writer.add_scalar(f'Train/{key}', value, epoch)
            self.writer.add_scalar('Train/learning_rate', self.scheduler.get_last_lr()[0] if self.scheduler else self.optimizer.param_groups[0]['lr'], epoch)
        
        return avg_metrics

    def evaluate(self, dataloader: DataLoader, epoch: int) -> Dict[str, float]:
        """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ã€‚"""
        total_metrics: Dict[str, float] = collections.defaultdict(float)
        num_batches = len(dataloader)
        progress_bar = tqdm(dataloader, desc=f"Evaluating Epoch {epoch}", disable=(self.rank not in [-1, 0]))
        
        with torch.no_grad():
            for batch in progress_bar:
                metrics = self._run_step(batch, is_train=False)
                for key, value in metrics.items(): total_metrics[key] += value
        
        avg_metrics = {key: value / num_batches for key, value in total_metrics.items()}
        
        if self.rank in [-1, 0]:
            print(f"Epoch {epoch} Validation Results: " + ", ".join([f"{k}: {v:.4f}" for k, v in avg_metrics.items()]))
            for key, value in avg_metrics.items():
                self.writer.add_scalar(f'Validation/{key}', value, epoch)
        
        return avg_metrics

    def save_checkpoint(self, path: str, epoch: int, metric_value: float, **kwargs: Any):
        if self.rank in [-1, 0]:
            model_to_save = self.model.module if isinstance(self.model, nn.parallel.DistributedDataParallel) else self.model
            
            # çŠ¶æ…‹(state)ãƒãƒƒãƒ•ã‚¡ã‚’é™¤ã„ãŸã€å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿å­˜
            model_state = {k: v for k, v in model_to_save.state_dict().items() if k not in model_to_save.buffers()}

            state = {
                'epoch': epoch, 'model_state_dict': model_state, 
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scaler_state_dict': self.scaler.state_dict(),
                'best_metric': self.best_metric
            }
            if self.scheduler: state['scheduler_state_dict'] = self.scheduler.state_dict()
            state.update(kwargs)
            
            os.makedirs(os.path.dirname(path), exist_ok=True)
            torch.save(state, path)
            print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ '{path}' ã«ä¿å­˜ã—ã¾ã—ãŸ (Epoch: {epoch})ã€‚")
            
            is_best = metric_value < self.best_metric
            if is_best:
                self.best_metric = metric_value
                best_path = os.path.join(os.path.dirname(path), 'best_model.pth')
                shutil.copyfile(path, best_path)
                print(f"ğŸ† æ–°ã—ã„ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ '{best_path}' ã«ä¿å­˜ã—ã¾ã—ãŸ (Metric: {metric_value:.4f})ã€‚")

    def load_checkpoint(self, path: str) -> int:
        if not os.path.exists(path):
            print(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}ã€‚æœ€åˆã‹ã‚‰å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            return 0
            
        map_location = {'cuda:%d' % 0: 'cuda:%d' % self.rank} if self.rank != -1 else self.device
        checkpoint = torch.load(path, map_location=map_location)
        
        model_to_load = self.model.module if isinstance(self.model, nn.parallel.DistributedDataParallel) else self.model
        # strict=Falseã«ã™ã‚‹ã“ã¨ã§ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«å­˜åœ¨ã—ãªã„ã‚­ãƒ¼(ãƒãƒƒãƒ•ã‚¡ãªã©)ãŒã‚ã£ã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„
        model_to_load.load_state_dict(checkpoint['model_state_dict'], strict=False)
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if self.scheduler and 'scheduler_state_dict' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        if self.use_amp and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])

        self.best_metric = checkpoint.get('best_metric', float('inf'))
        start_epoch = checkpoint.get('epoch', 0)
        print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ '{path}' ã‚’æ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚Epoch {start_epoch} ã‹ã‚‰å­¦ç¿’ã‚’å†é–‹ã—ã¾ã™ã€‚")
        return start_epoch


class DistillationTrainer(BreakthroughTrainer):
    """çŸ¥è­˜è’¸ç•™ã«ç‰¹åŒ–ã—ãŸãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»è©•ä¾¡æ©Ÿèƒ½å®Œå‚™ï¼‰ã€‚"""
    def _run_step(self, batch: Tuple[torch.Tensor, ...], is_train: bool) -> Dict[str, Any]:
        if is_train: self.model.train()
        else: self.model.eval()
            
        student_input, student_target, teacher_logits = [t.to(self.device) for t in batch]

        with torch.set_grad_enabled(is_train):
            with torch.cuda.amp.autocast(enabled=self.use_amp):
                student_logits, spike_data = self.model(student_input, return_spikes=True)
                assert isinstance(self.criterion, DistillationLoss)
                loss_dict = self.criterion(
                    student_logits=student_logits,
                    teacher_logits=teacher_logits,
                    targets=student_target,
                    spikes=spike_data
                )
        
        if is_train:
            self.optimizer.zero_grad()
            self.scaler.scale(loss_dict['total']).backward()
            if self.grad_clip_norm > 0:
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_norm)
            self.scaler.step(self.optimizer)
            self.scaler.update()
        
        return {k: v.cpu().item() if torch.is_tensor(v) else v for k, v in loss_dict.items()}
