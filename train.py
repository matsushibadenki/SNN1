# matsushibadenki/snn/train.py
# DIã‚³ãƒ³ãƒ†ãƒŠã‚’åˆ©ç”¨ã—ãŸã€çµ±åˆå­¦ç¿’å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ (ãƒ‡ãƒ¼ã‚¿å½¢å¼å‹•çš„å¯¾å¿œç‰ˆ)
#
# å¤‰æ›´ç‚¹:
# - SNN-Learning-Data-Format-Specification.md ã«åŸºã¥ãã€--data_formatå¼•æ•°ã‚’è¿½åŠ ã€‚
# - --data_format ã®å€¤ã«å¿œã˜ã¦ã€snn_research.data.datasetsã‹ã‚‰é©åˆ‡ãªDatasetã‚¯ãƒ©ã‚¹ã‚’å‹•çš„ã«èª­ã¿è¾¼ã‚€ã‚ˆã†ã«ä¿®æ­£ã€‚
# - snn_modelã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–æ™‚ã«ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ­£ã—ãæ¸¡ã™ã‚ˆã†ã«ä¿®æ­£ã€‚

import os
import argparse
import json
import torch
import torch.distributed as dist
from torch.utils.data import DataLoader, random_split, Dataset
from torch.utils.data.distributed import DistributedSampler
from torch.nn.utils.rnn import pad_sequence
from torch.nn.parallel import DistributedDataParallel as DDP
from functools import partial

from app.containers import TrainingContainer
from snn_research.data.datasets import DataFormat, Vocabulary, get_dataset_class

# --- PyTorchã®ç•°å¸¸æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ– ---
torch.autograd.set_detect_anomaly(True)

# --- (set_seed, collate_fn, DistillationDataset, distillation_collate_fn ã®ã‚³ãƒ¼ãƒ‰ã¯å¤‰æ›´ãªã—) ---
def set_seed(seed: int):
    import random
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def collate_fn(batch, pad_id):
    inputs, targets = zip(*batch)
    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=pad_id)
    padded_targets = pad_sequence(targets, batch_first=True, padding_value=pad_id)
    return padded_inputs, padded_targets

class DistillationDataset(Dataset):
    def __init__(self, file_path: str):
        with open(file_path, 'r', encoding='utf-8') as f:
            self.data = [json.loads(line)['text'] for line in f if line.strip()]
    def __len__(self): return len(self.data)
    def __getitem__(self, idx): return self.data[idx]

def distillation_collate_fn(batch_texts, student_vocab, teacher_tokenizer):
    student_inputs, student_targets = [], []
    for text in batch_texts:
        encoded = student_vocab.encode(text)
        student_inputs.append(torch.tensor(encoded[:-1]))
        student_targets.append(torch.tensor(encoded[1:], dtype=torch.long))
    
    student_padded_inputs = pad_sequence(student_inputs, batch_first=True, padding_value=student_vocab.pad_id)
    student_padded_targets = pad_sequence(student_targets, batch_first=True, padding_value=student_vocab.pad_id)

    teacher_tokenized = teacher_tokenizer(batch_texts, padding=True, truncation=True, max_length=128, return_tensors="pt")
    
    return student_padded_inputs, student_padded_targets, teacher_tokenized.input_ids, teacher_tokenized.attention_mask

def main_worker(rank, world_size, container, args):
    is_distributed = container.config.training.type() != "standard"
    is_distillation = container.config.training.type() == "distillation"
    
    if is_distributed:
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        dist.init_process_group("nccl", rank=rank, world_size=world_size)
        torch.cuda.set_device(rank)

    vocab_path = "vocab.pth"
    if rank in [-1, 0]:
        print("ğŸ“– èªå½™ã‚’æ§‹ç¯‰ä¸­...")
        vocab = container.vocabulary()
        
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å¿œã˜ã¦é©åˆ‡ãªDatasetã‚¯ãƒ©ã‚¹ã‚’å–å¾—
        data_format = DataFormat(container.config.data.format())
        dataset_class = get_dataset_class(data_format)
        
        # è’¸ç•™ã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ–¹æ³•ãŒç•°ãªã‚‹
        if is_distillation:
             # DistillationDatasetã¯ 'simple_text' å½¢å¼ã® {"text": "..."} ã‚’æƒ³å®š
            distillation_dataset = DistillationDataset(container.config.data.path())
            text_iterator = (item for item in distillation_dataset)
        else:
            text_iterator = dataset_class.extract_texts(container.config.data.path())

        vocab.build_vocab(text_iterator, max_size=container.config.data.max_vocab_size())
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        
        torch.save(vocab, vocab_path)
        print(f"âœ… èªå½™ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚èªå½™æ•°: {vocab.vocab_size}")

    if is_distributed: dist.barrier()
    vocab = torch.load(vocab_path, map_location='cpu', weights_only=False)

    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã¨å­¦ç¿’ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦Datasetã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    if is_distillation:
        dataset = DistillationDataset(container.config.data.path())
    else:
        data_format = DataFormat(container.config.data.format())
        dataset_class = get_dataset_class(data_format)
        dataset = dataset_class(container.config.data.path(), vocab)
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    
    val_split = int(len(dataset) * container.config.data.split_ratio())
    train_dataset, _ = random_split(dataset, [len(dataset) - val_split, val_split])
    
    sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True) if is_distributed else None
    
    _collate_fn = partial(collate_fn, pad_id=vocab.pad_id)
    if is_distillation:
        teacher_tokenizer = container.teacher_tokenizer()
        _collate_fn = partial(distillation_collate_fn, student_vocab=vocab, teacher_tokenizer=teacher_tokenizer)

    dataloader = DataLoader(train_dataset, batch_size=container.config.training.batch_size(),
                              sampler=sampler, collate_fn=_collate_fn, num_workers=2, shuffle=(sampler is None))

    # --- ãƒ‡ãƒã‚¤ã‚¹ã®è‡ªå‹•é¸æŠãƒ­ã‚¸ãƒƒã‚¯ ---
    if is_distributed:
        device = f"cuda:{rank}"
    else:
        config_device = container.config.device()
        if config_device == "cuda" and torch.cuda.is_available():
            device = "cuda"
        elif config_device == "mps" and torch.backends.mps.is_available():
            device = "mps"
        else:
            if config_device != "cpu":
                print(f"âš ï¸  '{config_device}' is not available. Falling back to 'cpu'.")
            device = "cpu"
    print(f"Selected device: {device}")
    
    # DIã‚³ãƒ³ãƒ†ãƒŠã«è¨­å®šæ¸ˆã¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã—ã€å‹•çš„ã«å¿…è¦ãªvocab_sizeã®ã¿ã‚’æ¸¡ã™
    model = container.snn_model(vocab_size=vocab.vocab_size).to(device)
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ç”¨ã«ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ç›´æ¥å–å¾—
    model_config = {
        'd_model': container.config.model.d_model(),
        'd_state': container.config.model.d_state(),
        'num_layers': container.config.model.num_layers(),
        'time_steps': container.config.model.time_steps(),
        'n_head': container.config.model.n_head(),
    }

    if is_distributed: model = DDP(model, device_ids=[rank])
    
    optimizer = container.optimizer(params=model.parameters())
    scheduler = container.scheduler(optimizer=optimizer) if container.config.training.use_scheduler() else None

    container.standard_loss.kwargs['pad_id'] = vocab.pad_id
    container.distillation_loss.kwargs['student_pad_id'] = vocab.pad_id
    
    # --- å­¦ç¿’ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦Trainerã‚’é¸æŠã—ã¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ– ---
    trainer_args = {
        "model": model, "optimizer": optimizer, "scheduler": scheduler,
        "device": device, "rank": rank,
    }
    if is_distillation:
        trainer = container.distillation_trainer(**trainer_args)
    else:
        trainer = container.standard_trainer(**trainer_args)

    if rank in [-1, 0]: print(f"\nğŸ”¥ {container.config.training.type()} å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    for epoch in range(container.config.training.epochs()):
        if is_distributed: sampler.set_epoch(epoch)
        metrics = trainer.train_epoch(dataloader)
        if rank in [-1, 0]:
            lr = scheduler.get_last_lr()[0] if scheduler else container.config.training.learning_rate()
            metrics_str = ", ".join([f"{k}: {v:.4f}" for k, v in metrics.items()])
            print(f"Epoch {epoch+1: >3}/{container.config.training.epochs()}: {metrics_str}, lr: {lr:.6f}")
            if (epoch + 1) % container.config.training.log_interval() == 0:
                trainer.save_checkpoint(
                    container.config.model.path(), 
                    epoch, 
                    vocab=vocab, 
                    config=model_config
                )

    if rank in [-1, 0] and os.path.exists(vocab_path): os.remove(vocab_path)
    if is_distributed: dist.destroy_process_group()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SNNãƒ¢ãƒ‡ãƒ«ã®çµ±åˆå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--data_path", type=str, help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ (è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã)")
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    parser.add_argument("--data_format", type=str, choices=[f.value for f in DataFormat], help="ãƒ‡ãƒ¼ã‚¿å½¢å¼ (è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã)")
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    args = parser.parse_args()

    container = TrainingContainer()
    container.config.from_yaml(args.config)
    if args.data_path: container.config.data.path.from_value(args.data_path)
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    if args.data_format: container.config.data.format.from_value(args.data_format)
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    
    set_seed(container.config.seed())

    training_type = container.config.training.type()
    if training_type in ["distributed", "distillation"]:
        world_size = torch.cuda.device_count()
        print(f"{world_size}å€‹ã®GPUã§ '{training_type}' å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        torch.multiprocessing.spawn(main_worker, args=(world_size, container, args), nprocs=world_size, join=True)
    else:
        device_name = container.config.device()
        if device_name == "cuda" and not torch.cuda.is_available():
            print("CUDA is not available, switching to MPS if possible, otherwise CPU.")
            device_name = "mps" if torch.backends.mps.is_available() else "cpu"
        elif device_name == "mps" and not torch.backends.mps.is_available():
            print("MPS is not available, switching to CPU.")
            device_name = "cpu"
        print(f"å˜ä¸€ãƒ‡ãƒã‚¤ã‚¹ ({device_name}) ã§ 'standard' å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        main_worker(-1, 1, container, args)
